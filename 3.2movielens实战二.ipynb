{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先来看看数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目使用的是MovieLens 1M 数据集，包含6000个用户在近4000部电影上的1亿条评论。\n",
    "\n",
    "数据集分为三个文件：用户数据users.dat，电影数据movies.dat和评分数据ratings.dat。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户数据\n",
    "分别有用户ID、性别、年龄、职业ID和邮编等字段。\n",
    "\n",
    "数据中的格式：UserID::Gender::Age::Occupation::Zip-code\n",
    "\n",
    "- Gender is denoted by a \"M\" for male and \"F\" for female\n",
    "- Age is chosen from the following ranges:\n",
    "\n",
    "\t*  1:  \"Under 18\"\n",
    "\t* 18:  \"18-24\"\n",
    "\t* 25:  \"25-34\"\n",
    "\t* 35:  \"35-44\"\n",
    "\t* 45:  \"45-49\"\n",
    "\t* 50:  \"50-55\"\n",
    "\t* 56:  \"56+\"\n",
    "\n",
    "- Occupation is chosen from the following choices:\n",
    "\n",
    "\t*  0:  \"other\" or not specified\n",
    "\t*  1:  \"academic/educator\"\n",
    "\t*  2:  \"artist\"\n",
    "\t*  3:  \"clerical/admin\"\n",
    "\t*  4:  \"college/grad student\"\n",
    "\t*  5:  \"customer service\"\n",
    "\t*  6:  \"doctor/health care\"\n",
    "\t*  7:  \"executive/managerial\"\n",
    "\t*  8:  \"farmer\"\n",
    "\t*  9:  \"homemaker\"\n",
    "\t* 10:  \"K-12 student\"\n",
    "\t* 11:  \"lawyer\"\n",
    "\t* 12:  \"programmer\"\n",
    "\t* 13:  \"retired\"\n",
    "\t* 14:  \"sales/marketing\"\n",
    "\t* 15:  \"scientist\"\n",
    "\t* 16:  \"self-employed\"\n",
    "\t* 17:  \"technician/engineer\"\n",
    "\t* 18:  \"tradesman/craftsman\"\n",
    "\t* 19:  \"unemployed\"\n",
    "\t* 20:  \"writer\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "print(users.shape)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出UserID、Gender、Age和Occupation都是类别字段，其中邮编字段是我们不使用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 电影数据\n",
    "分别有电影ID、电影名和电影风格等字段。\n",
    "\n",
    "数据中的格式：MovieID::Title::Genres\n",
    "\n",
    "- Titles are identical to titles provided by the IMDB (including\n",
    "year of release)\n",
    "- Genres are pipe-separated and are selected from the following genres:\n",
    "\n",
    "\t* Action\n",
    "\t* Adventure\n",
    "\t* Animation\n",
    "\t* Children's\n",
    "\t* Comedy\n",
    "\t* Crime\n",
    "\t* Documentary\n",
    "\t* Drama\n",
    "\t* Fantasy\n",
    "\t* Film-Noir\n",
    "\t* Horror\n",
    "\t* Musical\n",
    "\t* Mystery\n",
    "\t* Romance\n",
    "\t* Sci-Fi\n",
    "\t* Thriller\n",
    "\t* War\n",
    "\t* Western\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "print(movies.shape)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MovieID是类别字段，Title是文本，Genres也是类别字段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评分数据\n",
    "分别有用户ID、电影ID、评分和时间戳等字段。\n",
    "\n",
    "数据中的格式：UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "- UserIDs range between 1 and 6040 \n",
    "- MovieIDs range between 1 and 3952\n",
    "- Ratings are made on a 5-star scale (whole-star ratings only)\n",
    "- Timestamp is represented in seconds since the epoch as returned by time(2)\n",
    "- Each user has at least 20 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "print(ratings.shape)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 来说说数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UserID、Occupation和MovieID不用变。\n",
    "- Gender字段：需要将‘F’和‘M’转换成0和1。\n",
    "- Age字段：要转成7个连续数字0~6。\n",
    "- Genres字段：是分类字段，要转成数字。首先将Genres中的类别转成字符串到数字的字典，然后再将每个电影的Genres字段转成数字列表，因为有些电影是多个Genres的组合。\n",
    "- Title字段：处理方式跟Genres字段一样，首先创建文本到数字的字典，然后将Title中的描述转成数字的列表。另外Title中的年份也需要去掉。\n",
    "- Genres和Title字段需要将长度统一，这样在神经网络中方便处理。空白部分用‘< PAD >’对应的数字填充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20\n",
       "5       6       0    3      9\n",
       "6       7       1    1      1\n",
       "7       8       1    6     12\n",
       "8       9       1    6     17\n",
       "9      10       0    1      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用户数据预处理\n",
    "users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "#将Zip-code过滤掉\n",
    "users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "#users_orig是没有做数据处理的原始用户数据\n",
    "users_orig = users.values\n",
    "#改变User数据中性别和年龄(女性为0，男性为1)\n",
    "gender_map = {'F':0, 'M':1}\n",
    "users['Gender'] = users['Gender'].map(gender_map)\n",
    "    \n",
    "#set函数是将其去重，然后通过enumerate函数进行遍历，ii是序号，val是值\n",
    "#将7种不同的年龄分别对应0~6(不是有序排列，也就是说并不是年龄小的在前面，年龄大的在后面)\n",
    "age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "users['Age'] = users['Age'].map(age_map)\n",
    "users.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5215\n",
      "[[1 'Toy Story (1995)' \"Animation|Children's|Comedy\"]\n",
      " [2 'Jumanji (1995)' \"Adventure|Children's|Fantasy\"]\n",
      " [3 'Grumpier Old Men (1995)' 'Comedy|Romance']\n",
      " ...\n",
      " [3950 'Tigerland (2000)' 'Drama']\n",
      " [3951 'Two Family House (2000)' 'Drama']\n",
      " [3952 'Contender, The (2000)' 'Drama|Thriller']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[2180, 2820, 1227, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[12, 5, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[2894, 1227, 1227, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[17, 5, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[2151, 2345, 4902, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[2, 16, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[2469, 2251, 4181, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[2, 18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[4038, 824, 2249, 3169, 1658, 2452, 1227, 1227...</td>\n",
       "      <td>[2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>[4087, 1227, 1227, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[9, 7, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>[3295, 1227, 1227, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[2, 16, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>[4714, 1073, 2080, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[17, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>[1494, 605, 1227, 1227, 1227, 1227, 1227, 1227...</td>\n",
       "      <td>[9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[3177, 1227, 1227, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[9, 17, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [2180, 2820, 1227, 1227, 1227, 1227, 1227, 122...   \n",
       "1        2  [2894, 1227, 1227, 1227, 1227, 1227, 1227, 122...   \n",
       "2        3  [2151, 2345, 4902, 1227, 1227, 1227, 1227, 122...   \n",
       "3        4  [2469, 2251, 4181, 1227, 1227, 1227, 1227, 122...   \n",
       "4        5  [4038, 824, 2249, 3169, 1658, 2452, 1227, 1227...   \n",
       "5        6  [4087, 1227, 1227, 1227, 1227, 1227, 1227, 122...   \n",
       "6        7  [3295, 1227, 1227, 1227, 1227, 1227, 1227, 122...   \n",
       "7        8  [4714, 1073, 2080, 1227, 1227, 1227, 1227, 122...   \n",
       "8        9  [1494, 605, 1227, 1227, 1227, 1227, 1227, 1227...   \n",
       "9       10  [3177, 1227, 1227, 1227, 1227, 1227, 1227, 122...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [12, 5, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "1  [17, 5, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "2  [2, 16, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "3  [2, 18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "4  [2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
       "5  [9, 7, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
       "6  [2, 16, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "7  [17, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "8  [9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
       "9  [9, 17, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#电影数据预处理\n",
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "#movies_orig是没有做数据处理的原始电影数据\n",
    "movies_orig = movies.values\n",
    "#将Title中的年份去掉（正则表达式）\n",
    "pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "\n",
    "title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "#电影类型转数字字典\n",
    "#genres_set.update(val)是将val合并到genres_set中，相同的元素只会出现一次\n",
    "genres_set = set()\n",
    "for val in movies['Genres'].str.split('|'):\n",
    "    genres_set.update(val)\n",
    "#genres_set是得出所有的电影种类\n",
    "genres_set.add('<PAD>')\n",
    "#genres2int是给电影种类编号的字典（通过值查找编号）\n",
    "genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "#将电影类型转成等长数字列表，如'Comedy|Horror': [7, 15]\n",
    "genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "#为其添加填充值，使得它的长度为18（因为有的电影所属的类别数量不一样，为了做到统一）\n",
    "for key in genres_map:\n",
    "    #max(genres2int.values())就是18\n",
    "    for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "        #这是在它的后面从第len个位置开始插入'<PAD>'的编号\n",
    "        genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "    \n",
    "movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "#电影Title转数字字典\n",
    "title_set = set()\n",
    "for val in movies['Title'].str.split():\n",
    "    title_set.update(val)\n",
    "    \n",
    "title_set.add('<PAD>')\n",
    "#获取title中的所有的词，并给它编号（title文本的集合）\n",
    "title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "#print(max(title2int.values()))\n",
    "print(len(title_set))\n",
    "\n",
    "#将电影Title转成等长数字列表，长度是15（因为不同的title的词的长度不一样,取15是因为几乎没有电影的名字有15个词）\n",
    "title_count = 15\n",
    "title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "    \n",
    "for key in title_map:\n",
    "    for cnt in range(title_count - len(title_map[key])):\n",
    "        #这是在title词向量的后面插入'<PAD>'的编号\n",
    "        title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "#类似于电影类型\n",
    "movies['Title'] = movies['Title'].map(title_map)\n",
    "print(movies_orig)\n",
    "movies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1197</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1287</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2804</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>594</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>919</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  ratings\n",
       "0       1     1193        5\n",
       "1       1      661        3\n",
       "2       1      914        3\n",
       "3       1     3408        4\n",
       "4       1     2355        5\n",
       "5       1     1197        3\n",
       "6       1     1287        5\n",
       "7       1     2804        5\n",
       "8       1      594        4\n",
       "9       1      919        4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#评分数据预处理\n",
    "ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "#将时间戳过滤掉\n",
    "ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "ratings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         UserID  MovieID  Gender  Age  JobID  \\\n",
      "0             1     1193       0    0     10   \n",
      "1             2     1193       1    5     16   \n",
      "2            12     1193       1    6     12   \n",
      "3            15     1193       1    6      7   \n",
      "4            17     1193       1    3      1   \n",
      "...         ...      ...     ...  ...    ...   \n",
      "1000204    5949     2198       1    4     17   \n",
      "1000205    5675     2703       1    1     14   \n",
      "1000206    5780     2845       1    4     17   \n",
      "1000207    5851     3607       0    4     20   \n",
      "1000208    5938     2909       1    6      1   \n",
      "\n",
      "                                                     Title  \\\n",
      "0        [3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...   \n",
      "1        [3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...   \n",
      "2        [3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...   \n",
      "3        [3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...   \n",
      "4        [3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...   \n",
      "...                                                    ...   \n",
      "1000204  [3389, 1227, 1227, 1227, 1227, 1227, 1227, 122...   \n",
      "1000205  [1397, 3061, 1227, 1227, 1227, 1227, 1227, 122...   \n",
      "1000206  [2853, 3898, 1227, 1227, 1227, 1227, 1227, 122...   \n",
      "1000207  [3241, 4289, 1290, 1227, 1227, 1227, 1227, 122...   \n",
      "1000208  [1516, 1349, 3122, 644, 1073, 4932, 1227, 1227...   \n",
      "\n",
      "                                                    Genres  \n",
      "0        [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
      "1        [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
      "2        [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
      "3        [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
      "4        [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
      "...                                                    ...  \n",
      "1000204  [13, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
      "1000205  [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
      "1000206  [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
      "1000207  [2, 18, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
      "1000208  [13, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
      "\n",
      "[1000209 rows x 7 columns]\n",
      "\n",
      "\n",
      "         ratings\n",
      "0              5\n",
      "1              5\n",
      "2              4\n",
      "3              4\n",
      "4              5\n",
      "...          ...\n",
      "1000204        5\n",
      "1000205        3\n",
      "1000206        1\n",
      "1000207        5\n",
      "1000208        4\n",
      "\n",
      "[1000209 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>ratings</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...</td>\n",
       "      <td>[18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>[3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...</td>\n",
       "      <td>[18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>[3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...</td>\n",
       "      <td>[18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>[3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...</td>\n",
       "      <td>[18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...</td>\n",
       "      <td>[18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000204</td>\n",
       "      <td>5949</td>\n",
       "      <td>2198</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>[3389, 1227, 1227, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[13, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000205</td>\n",
       "      <td>5675</td>\n",
       "      <td>2703</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>[1397, 3061, 1227, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000206</td>\n",
       "      <td>5780</td>\n",
       "      <td>2845</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>[2853, 3898, 1227, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000207</td>\n",
       "      <td>5851</td>\n",
       "      <td>3607</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>[3241, 4289, 1290, 1227, 1227, 1227, 1227, 122...</td>\n",
       "      <td>[2, 18, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000208</td>\n",
       "      <td>5938</td>\n",
       "      <td>2909</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[1516, 1349, 3122, 644, 1073, 4932, 1227, 1227...</td>\n",
       "      <td>[13, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID  MovieID  ratings  Gender  Age  JobID  \\\n",
       "0             1     1193        5       0    0     10   \n",
       "1             2     1193        5       1    5     16   \n",
       "2            12     1193        4       1    6     12   \n",
       "3            15     1193        4       1    6      7   \n",
       "4            17     1193        5       1    3      1   \n",
       "...         ...      ...      ...     ...  ...    ...   \n",
       "1000204    5949     2198        5       1    4     17   \n",
       "1000205    5675     2703        3       1    1     14   \n",
       "1000206    5780     2845        1       1    4     17   \n",
       "1000207    5851     3607        5       0    4     20   \n",
       "1000208    5938     2909        4       1    6      1   \n",
       "\n",
       "                                                     Title  \\\n",
       "0        [3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...   \n",
       "1        [3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...   \n",
       "2        [3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...   \n",
       "3        [3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...   \n",
       "4        [3241, 2445, 620, 2249, 424, 617, 1227, 1227, ...   \n",
       "...                                                    ...   \n",
       "1000204  [3389, 1227, 1227, 1227, 1227, 1227, 1227, 122...   \n",
       "1000205  [1397, 3061, 1227, 1227, 1227, 1227, 1227, 122...   \n",
       "1000206  [2853, 3898, 1227, 1227, 1227, 1227, 1227, 122...   \n",
       "1000207  [3241, 4289, 1290, 1227, 1227, 1227, 1227, 122...   \n",
       "1000208  [1516, 1349, 3122, 644, 1073, 4932, 1227, 1227...   \n",
       "\n",
       "                                                    Genres  \n",
       "0        [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "1        [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "2        [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "3        [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "4        [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "...                                                    ...  \n",
       "1000204  [13, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "1000205  [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "1000206  [18, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "1000207  [2, 18, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "1000208  [13, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,...  \n",
       "\n",
       "[1000209 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#合并三个表\n",
    "data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    \n",
    "#将数据分成X和y两张表\n",
    "target_fields = ['ratings']\n",
    "#分为特征表和目标表（特征表剔除了评分数据，目标表保留了评分数据）\n",
    "features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "features = features_pd.values\n",
    "targets_values = targets_pd.values\n",
    "    \n",
    "print(features_pd)\n",
    "print('\\n')\n",
    "print(targets_pd)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据并保存到本地"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- title_count：Title字段的长度（15）\n",
    "- title_set：Title文本的集合\n",
    "- genres2int：电影类型转数字的字典\n",
    "- features：是输入X\n",
    "- targets_values：是学习目标y\n",
    "- ratings：评分数据集的Pandas对象\n",
    "- users：用户数据集的Pandas对象\n",
    "- movies：电影数据的Pandas对象\n",
    "- data：三个数据集组合在一起的Pandas对象\n",
    "- movies_orig：没有做数据处理的原始电影数据\n",
    "- users_orig：没有做数据处理的原始用户数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#为方便调用，将其写在一个函数中\n",
    "def load_data():\n",
    "    #用户数据预处理\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "    #将Zip-code过滤掉\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    #获取值，过滤掉标题\n",
    "    users_orig = users.values\n",
    "    #改变User数据中性别和年龄(女性为0，男性为1)\n",
    "    gender_map = {'F':0, 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "    \n",
    "    #set函数是将其去重，然后通过enumerate函数进行遍历，ii是序号，val是值\n",
    "    #将7种不同的年龄分别对应0~6(不是有序排列，也就是说并不是年龄小的在前面，年龄大的在后面)\n",
    "    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "    \n",
    "    \n",
    "    #电影数据预处理\n",
    "    movies_title = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "    movies_orig = movies.values\n",
    "    #将Title中的年份去掉（正则表达式）\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "\n",
    "    title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #电影类型转数字字典\n",
    "    #genres_set.update(val)是将val合并到genres_set中，相同的元素只会出现一次\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "    #genres_set是得出所有的电影种类\n",
    "    genres_set.add('<PAD>')\n",
    "    #genres2int是给电影种类编号的字典（通过值查找编号）\n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "    #将电影类型转成等长数字列表，如'Comedy|Horror': [7, 15]\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "    #为其添加填充值，使得它的长度为18（因为有的电影所属的类别数量不一样，为了做到统一）\n",
    "    for key in genres_map:\n",
    "        #max(genres2int.values())就是18\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            #这是在它的后面从第len个位置开始插入'<PAD>'的编号\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "\n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "    #电影Title转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "\n",
    "    title_set.add('<PAD>')\n",
    "    #获取title中的所有的词，并给它编号（title文本的集合）\n",
    "    title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "\n",
    "    #将电影Title转成等长数字列表，长度是15（因为不同的title的词的长度不一样,取15是因为几乎没有电影的名字有15个词）\n",
    "    title_count = 15\n",
    "    title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "\n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            #这是在title词向量的后面插入'<PAD>'的编号\n",
    "            title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "    #类似于电影类型\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "    \n",
    "    \n",
    "    #评分数据预处理\n",
    "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    #将时间戳过滤掉\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "    \n",
    "    \n",
    "    #合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "\n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    #分为特征表和目标表（特征表剔除了评分数据，目标表保留了评分数据）\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    #剔除了评分数据的特征表的值\n",
    "    features = features_pd.values\n",
    "    #由评分数据组成的目标表的值\n",
    "    targets_values = targets_pd.values\n",
    "\n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据\n",
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "#将对象obj保存到文件file中去\n",
    "#pickle.dump用于保存文件\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('./save_data/preprocess.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从本地文件中读取数据\n",
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('./save_data/preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/model.001.jpeg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过研究数据集中的字段类型，我们发现有一些是类别字段，通常的处理是将这些字段转成one hot编码，但是像UserID、MovieID这样的字段就会变成非常的稀疏，输入的维度急剧膨胀，这是我们不愿意见到的，毕竟我这小笔记本不像大厂动辄能处理数以亿计维度的输入：）\n",
    "\n",
    "所以在预处理数据时将这些字段转成了数字，我们用这个数字当做嵌入矩阵的索引，在网络的第一层使用了嵌入层，维度是（N，32）和（N，16）。\n",
    "\n",
    "电影类型的处理要多一步，有时一个电影有多个电影类型，这样从嵌入矩阵索引出来是一个（n，32）的矩阵，因为有多个类型嘛，我们要将这个矩阵求和，变成（1，32）的向量。\n",
    "\n",
    "电影名的处理比较特殊，没有使用循环神经网络，而是用了文本卷积网络，下文会进行说明。\n",
    "\n",
    "从嵌入层索引出特征以后，将各特征传入全连接层，将输出再次传入全连接层，最终分别得到（1，200）的用户特征和电影特征两个特征向量。\n",
    "\n",
    "我们的目的就是要训练出用户特征和电影特征，在实现推荐功能时使用。得到这两个特征以后，就可以选择任意的方式来拟合评分了。我使用了两种方式，一个是上图中画出的将两个特征做向量乘法，将结果与真实评分做回归，采用MSE优化损失。因为本质上这是一个回归问题，另一种方式是，将两个特征作为输入，再次传入全连接层，输出一个值，将输出值回归到真实评分，采用MSE优化损失。\n",
    "\n",
    "实际上第二个方式的MSE loss在0.8附近，第一个方式在1附近，5次迭代的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本卷积网络\n",
    "网络看起来像下面这样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/text_cnn.png\"/>\n",
    "图片来自Kim Yoon的论文：[`Convolutional Neural Networks for Sentence Classification`](https://arxiv.org/abs/1408.5882)\n",
    "\n",
    "将卷积神经网络用于文本的文章建议你阅读[`Understanding Convolutional Neural Networks for NLP`](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络的第一层是词嵌入层，由每一个单词的嵌入向量组成的嵌入矩阵。下一层使用多个不同尺寸（窗口大小）的卷积核在嵌入矩阵上做卷积，窗口大小指的是每次卷积覆盖几个单词。这里跟对图像做卷积不太一样，图像的卷积通常用2x2、3x3、5x5之类的尺寸，而文本卷积要覆盖整个单词的嵌入向量，所以尺寸是（单词数，向量维度），比如每次滑动3个，4个或者5个单词。第三层网络是max pooling得到一个长向量，最后使用dropout做正则化，最终得到了电影Title的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#将参数保存到文件中\n",
    "def save_params(params):\n",
    "    pickle.dump(params, open('./save_data/params.p', 'wb'))\n",
    "\n",
    "#从文件中读取参数\n",
    "def load_params():\n",
    "    return pickle.load(open('./save_data/params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "#用户ID个数\n",
    "#ndarray.take(indices, axis=None, out=None, mode='raise')\n",
    "#按axis选择处于indices位置上的值\n",
    "#axis用于选择值的轴，0为横轴，1为纵向选\n",
    "#如features.take(0,0)就会选择横向第一条数据，(1,0)会选择横向第二条数据\n",
    "#因为是从0开始，所以要+1\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "#性别个数\n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "\n",
    "#电影ID个数\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3883\n",
    "#电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "movie_title_max = len(title_set) # 5215\n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 15\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "#另外91号电影并不在数据集中，要考虑存在空缺值的情况\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#循环次数\n",
    "num_epochs = 5\n",
    "#每个批次的大小\n",
    "batch_size = 256\n",
    "\n",
    "#剔除神经元的比例\n",
    "dropout_keep = 0.5\n",
    "#学习率\n",
    "learning_rate = 0.0001\n",
    "#显示每n个批次的统计数据（每n个批次打印一下数据）\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save_model/save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义输入的占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    \n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    #电影的种类，N行18列\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "    #电影的名字，N行15列\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
    "    #评分\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义User的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    with tf.name_scope(\"user_embedding\"):\n",
    "        #tf.Variable(initializer,name)：initializer是初始化参数，可以有tf.random_normal,tf.constant等,name是变量的名字\n",
    "        #tf.random_uniform(shape, minval=0,maxval=None,dtype=tf.float32) 从均匀分布中输出随机值。\n",
    "        #返回shape形状矩阵：用户数uid_max×特征数embed_dim，产生于low(-1)和high(1)之间，产生的值是均匀分布的。\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
    "        #tf.nn.embedding_lookup(tensor, id) 选取一个张量tensor里面索引id对应的元素\n",
    "        #选取uid_embed_matrix（用户ID嵌入矩阵）中的用户id对应的某个用户id的向量\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "        \n",
    "        #特征数降一半\n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        #选取gender_embed_matrix（用户性别嵌入矩阵）的用户性别对应的某个用户性别的向量\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        \n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        \n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将User的嵌入矩阵一起全连接生成User的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #构建各属性的全连接层\n",
    "        # tf.layers.dense(inputs,units,activation=None,use_bias=True,kernel_initializer=None,bias_initializer=tf.zeros_initializer(),\n",
    "        # kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,\n",
    "        #bias_constraint=None,trainable=True,name=None,reuse=None)\n",
    "        #inputs:该层的输入; units:输出的大小(维数),整数或long; activation: 使用什么激活函数（神经网络的非线性层），默认为None，不使用激活函数\n",
    "        #name该层的名字\n",
    "        #即以用户ID嵌入层作为输入，输出维度为嵌入矩阵的维度，采用relu激活函数，dense是新增加一层。\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)\n",
    "        #以用户性别嵌入层作为输入，输出维度为嵌入矩阵的维度\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "        \n",
    "        #第一层全连接\n",
    "        #tf.concat(values,axis,name)是连接两个矩阵的操作，本身不会增加维度，返回的是连接后的tensor.\n",
    "        #values应该是一个tensor的list或者tuple。\n",
    "        #axis则是我们想要连接的维度。对于二维来说0表示第一个括号维度，1表示第二个括号维度\n",
    "        #t1 = [[1, 2, 3], [4, 5, 6]]\n",
    "        #t2 = [[7, 8, 9], [10, 11, 12]]\n",
    "        #tf.concat([t1, t2], 0)表示为 [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
    "        #tf.concat([t1, t2], 1)表示为 [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]\n",
    "        #对于三维来说0表示第一个括号维度，1表示第二个括号维度，2表示第三个括号维度\n",
    "        #t1 = [ [[1],[2]] , [[3],[4]] ]\n",
    "        #t2 = [ [[5],[6]] , [[7],[8]] ]\n",
    "        #tf.concat([t1, t2], 0)表示为[[[1],[2]] , [[3],[4]] , [[5],[6]] , [[7],[8]]]\n",
    "        #tf.concat([t1, t2], 1)表示为[[[1],[2],[5],[6]],[[3],[4],[7],[8]]]\n",
    "        #tf.concat([t1, t2], 2)表示为[[[1 5],[2 6]],[[3 7],[4 8]]]\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)  4*32=128\n",
    "        #tf.contrib.layers.fully_connected（F输入, num_outputs,activation_fn）用于构建全连接层\n",
    "        #第二层全连接\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "        #-1表示缺省值，满足其他维度要求，这里该是几就是几，(?, 200）\n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义Movie ID的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对电影类型的多个嵌入向量做加和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        #选取电影种类嵌入矩阵中对应的某个电影种类的向量，也是一个32维的向量\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\")\n",
    "        if combiner == \"sum\":\n",
    "             #tf.reduce_sum(input_tensor,axis=None,keep_dims=False,name=None,reduction_indices=None)要输出看！\n",
    "             #压缩求和，用于降维\n",
    "             #函数中的input_tensor是按照axis中已经给定的维度来减少的，axis表示按第几个维度求和\n",
    "             #但是keep_dims为true，则维度不会减小\n",
    "             #如果axis没有条目，则缩小所有维度，并返回具有单个元素的张量\n",
    "             #'x' is [[1, 1, 1]\n",
    "             # [1, 1, 1]]\n",
    "             #求和tf.reduce_sum(x) ==> 6\n",
    "             #按列求和tf.reduce_sum(x, 0) ==> [2, 2, 2]（即得到行结果）\n",
    "             #按行求和tf.reduce_sum(x, 1) ==> [3, 3] （即得到列结果）\n",
    "             #按照行的维度求和tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]（维度不会减少）\n",
    "             #x现在一行为一个类型向量，应该按axis=1进行sum\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    #     elif combiner == \"mean\":\n",
    "\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Title的文本卷积网络实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        #得到电影标题词嵌入矩阵，维度为32维\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
    "        #movie_title_embed_layer从电影标题词嵌入矩阵中选取某个电影标题词的向量\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    \n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    #window_sizes = {2, 3, 4, 5}，分别取2,3,4,5这四个步长\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            \n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "            \n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "\n",
    "    #Dropout层\n",
    "    with tf.name_scope(\"pool_dropout\"):\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "    \n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将Movie的各个层一起做全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #第一层全连接\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        #第二层全连接\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-18-fbddb7d4e7bc>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-20-c9b8f82b2265>:19: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-21-4491a2389001>:30: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From d:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "# tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #获取User的4个嵌入向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    #得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    #获取电影类型的嵌入向量\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "    #得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
    "                                                                                movie_categories_embed_layer, \n",
    "                                                                                dropout_layer)\n",
    "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "#         inference_layer = tf.concat([user_combine_layer_flat, movie_combine_layer_flat], 1)  #(?, 200)\n",
    "#         inference = tf.layers.dense(inference_layer, 1,\n",
    "#                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01), \n",
    "#                                     kernel_regularizer=tf.nn.l2_loss, name=\"inference\")\n",
    "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "#        inference = tf.matmul(user_combine_layer_flat, tf.transpose(movie_combine_layer_flat))\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    # 优化损失 \n",
    "#     train_op = tf.train.AdamOptimizer(lr).minimize(loss)  #cost\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to E:\\jupyter\\RecommendSystem\\runs\\1571488305\n",
      "\n",
      "2019-10-19T20:31:48.840528: Epoch   0 Batch    0/3125   train_loss = 8.713\n",
      "2019-10-19T20:31:49.328516: Epoch   0 Batch   20/3125   train_loss = 3.891\n",
      "2019-10-19T20:31:49.814214: Epoch   0 Batch   40/3125   train_loss = 3.010\n",
      "2019-10-19T20:31:50.280874: Epoch   0 Batch   60/3125   train_loss = 2.304\n",
      "2019-10-19T20:31:50.739314: Epoch   0 Batch   80/3125   train_loss = 2.027\n",
      "2019-10-19T20:31:51.210808: Epoch   0 Batch  100/3125   train_loss = 1.917\n",
      "2019-10-19T20:31:51.708236: Epoch   0 Batch  120/3125   train_loss = 1.784\n",
      "2019-10-19T20:31:52.163104: Epoch   0 Batch  140/3125   train_loss = 1.649\n",
      "2019-10-19T20:31:52.637939: Epoch   0 Batch  160/3125   train_loss = 1.499\n",
      "2019-10-19T20:31:53.150723: Epoch   0 Batch  180/3125   train_loss = 1.327\n",
      "2019-10-19T20:31:53.649943: Epoch   0 Batch  200/3125   train_loss = 1.624\n",
      "2019-10-19T20:31:54.147729: Epoch   0 Batch  220/3125   train_loss = 1.539\n",
      "2019-10-19T20:31:54.613045: Epoch   0 Batch  240/3125   train_loss = 1.358\n",
      "2019-10-19T20:31:55.099849: Epoch   0 Batch  260/3125   train_loss = 1.500\n",
      "2019-10-19T20:31:55.582403: Epoch   0 Batch  280/3125   train_loss = 1.567\n",
      "2019-10-19T20:31:56.055463: Epoch   0 Batch  300/3125   train_loss = 1.405\n",
      "2019-10-19T20:31:56.528200: Epoch   0 Batch  320/3125   train_loss = 1.432\n",
      "2019-10-19T20:31:56.994144: Epoch   0 Batch  340/3125   train_loss = 1.292\n",
      "2019-10-19T20:31:57.448047: Epoch   0 Batch  360/3125   train_loss = 1.377\n",
      "2019-10-19T20:31:57.939914: Epoch   0 Batch  380/3125   train_loss = 1.377\n",
      "2019-10-19T20:31:58.438519: Epoch   0 Batch  400/3125   train_loss = 1.192\n",
      "2019-10-19T20:31:58.935209: Epoch   0 Batch  420/3125   train_loss = 1.134\n",
      "2019-10-19T20:31:59.412630: Epoch   0 Batch  440/3125   train_loss = 1.323\n",
      "2019-10-19T20:31:59.901158: Epoch   0 Batch  460/3125   train_loss = 1.369\n",
      "2019-10-19T20:32:00.361649: Epoch   0 Batch  480/3125   train_loss = 1.399\n",
      "2019-10-19T20:32:00.843161: Epoch   0 Batch  500/3125   train_loss = 1.051\n",
      "2019-10-19T20:32:01.324280: Epoch   0 Batch  520/3125   train_loss = 1.375\n",
      "2019-10-19T20:32:01.807214: Epoch   0 Batch  540/3125   train_loss = 1.171\n",
      "2019-10-19T20:32:02.292435: Epoch   0 Batch  560/3125   train_loss = 1.370\n",
      "2019-10-19T20:32:02.763681: Epoch   0 Batch  580/3125   train_loss = 1.523\n",
      "2019-10-19T20:32:03.232785: Epoch   0 Batch  600/3125   train_loss = 1.390\n",
      "2019-10-19T20:32:03.695628: Epoch   0 Batch  620/3125   train_loss = 1.383\n",
      "2019-10-19T20:32:04.178622: Epoch   0 Batch  640/3125   train_loss = 1.400\n",
      "2019-10-19T20:32:04.656050: Epoch   0 Batch  660/3125   train_loss = 1.297\n",
      "2019-10-19T20:32:05.127835: Epoch   0 Batch  680/3125   train_loss = 1.161\n",
      "2019-10-19T20:32:05.605172: Epoch   0 Batch  700/3125   train_loss = 1.250\n",
      "2019-10-19T20:32:06.068686: Epoch   0 Batch  720/3125   train_loss = 1.186\n",
      "2019-10-19T20:32:06.532636: Epoch   0 Batch  740/3125   train_loss = 1.321\n",
      "2019-10-19T20:32:07.008406: Epoch   0 Batch  760/3125   train_loss = 1.328\n",
      "2019-10-19T20:32:07.491463: Epoch   0 Batch  780/3125   train_loss = 1.355\n",
      "2019-10-19T20:32:07.945411: Epoch   0 Batch  800/3125   train_loss = 1.189\n",
      "2019-10-19T20:32:08.402356: Epoch   0 Batch  820/3125   train_loss = 1.275\n",
      "2019-10-19T20:32:08.868429: Epoch   0 Batch  840/3125   train_loss = 1.154\n",
      "2019-10-19T20:32:09.341904: Epoch   0 Batch  860/3125   train_loss = 1.203\n",
      "2019-10-19T20:32:09.801521: Epoch   0 Batch  880/3125   train_loss = 1.175\n",
      "2019-10-19T20:32:10.273861: Epoch   0 Batch  900/3125   train_loss = 1.247\n",
      "2019-10-19T20:32:10.725977: Epoch   0 Batch  920/3125   train_loss = 1.215\n",
      "2019-10-19T20:32:11.229087: Epoch   0 Batch  940/3125   train_loss = 1.415\n",
      "2019-10-19T20:32:11.701940: Epoch   0 Batch  960/3125   train_loss = 1.368\n",
      "2019-10-19T20:32:12.153111: Epoch   0 Batch  980/3125   train_loss = 1.306\n",
      "2019-10-19T20:32:12.617086: Epoch   0 Batch 1000/3125   train_loss = 1.277\n",
      "2019-10-19T20:32:13.084208: Epoch   0 Batch 1020/3125   train_loss = 1.355\n",
      "2019-10-19T20:32:13.550067: Epoch   0 Batch 1040/3125   train_loss = 1.227\n",
      "2019-10-19T20:32:14.040807: Epoch   0 Batch 1060/3125   train_loss = 1.405\n",
      "2019-10-19T20:32:14.521551: Epoch   0 Batch 1080/3125   train_loss = 1.222\n",
      "2019-10-19T20:32:14.989213: Epoch   0 Batch 1100/3125   train_loss = 1.365\n",
      "2019-10-19T20:32:15.455523: Epoch   0 Batch 1120/3125   train_loss = 1.213\n",
      "2019-10-19T20:32:15.921083: Epoch   0 Batch 1140/3125   train_loss = 1.210\n",
      "2019-10-19T20:32:16.432560: Epoch   0 Batch 1160/3125   train_loss = 1.253\n",
      "2019-10-19T20:32:16.888341: Epoch   0 Batch 1180/3125   train_loss = 1.304\n",
      "2019-10-19T20:32:17.356154: Epoch   0 Batch 1200/3125   train_loss = 1.205\n",
      "2019-10-19T20:32:17.814547: Epoch   0 Batch 1220/3125   train_loss = 1.187\n",
      "2019-10-19T20:32:18.277264: Epoch   0 Batch 1240/3125   train_loss = 1.134\n",
      "2019-10-19T20:32:18.729300: Epoch   0 Batch 1260/3125   train_loss = 1.205\n",
      "2019-10-19T20:32:19.195045: Epoch   0 Batch 1280/3125   train_loss = 1.178\n",
      "2019-10-19T20:32:19.646181: Epoch   0 Batch 1300/3125   train_loss = 1.221\n",
      "2019-10-19T20:32:20.106906: Epoch   0 Batch 1320/3125   train_loss = 1.224\n",
      "2019-10-19T20:32:20.579414: Epoch   0 Batch 1340/3125   train_loss = 1.085\n",
      "2019-10-19T20:32:21.085695: Epoch   0 Batch 1360/3125   train_loss = 1.106\n",
      "2019-10-19T20:32:21.561928: Epoch   0 Batch 1380/3125   train_loss = 0.998\n",
      "2019-10-19T20:32:22.019569: Epoch   0 Batch 1400/3125   train_loss = 1.280\n",
      "2019-10-19T20:32:22.501671: Epoch   0 Batch 1420/3125   train_loss = 1.224\n",
      "2019-10-19T20:32:22.962509: Epoch   0 Batch 1440/3125   train_loss = 1.184\n",
      "2019-10-19T20:32:23.425458: Epoch   0 Batch 1460/3125   train_loss = 1.299\n",
      "2019-10-19T20:32:23.882823: Epoch   0 Batch 1480/3125   train_loss = 1.200\n",
      "2019-10-19T20:32:24.351743: Epoch   0 Batch 1500/3125   train_loss = 1.292\n",
      "2019-10-19T20:32:24.842531: Epoch   0 Batch 1520/3125   train_loss = 1.214\n",
      "2019-10-19T20:32:25.322731: Epoch   0 Batch 1540/3125   train_loss = 1.288\n",
      "2019-10-19T20:32:25.807904: Epoch   0 Batch 1560/3125   train_loss = 1.212\n",
      "2019-10-19T20:32:26.316630: Epoch   0 Batch 1580/3125   train_loss = 1.222\n",
      "2019-10-19T20:32:26.786943: Epoch   0 Batch 1600/3125   train_loss = 1.275\n",
      "2019-10-19T20:32:27.258055: Epoch   0 Batch 1620/3125   train_loss = 1.224\n",
      "2019-10-19T20:32:27.722413: Epoch   0 Batch 1640/3125   train_loss = 1.259\n",
      "2019-10-19T20:32:28.191068: Epoch   0 Batch 1660/3125   train_loss = 1.283\n",
      "2019-10-19T20:32:28.671684: Epoch   0 Batch 1680/3125   train_loss = 1.146\n",
      "2019-10-19T20:32:29.170597: Epoch   0 Batch 1700/3125   train_loss = 1.095\n",
      "2019-10-19T20:32:29.629703: Epoch   0 Batch 1720/3125   train_loss = 1.159\n",
      "2019-10-19T20:32:30.089850: Epoch   0 Batch 1740/3125   train_loss = 1.205\n",
      "2019-10-19T20:32:30.561864: Epoch   0 Batch 1760/3125   train_loss = 1.286\n",
      "2019-10-19T20:32:31.028536: Epoch   0 Batch 1780/3125   train_loss = 1.081\n",
      "2019-10-19T20:32:31.497873: Epoch   0 Batch 1800/3125   train_loss = 1.205\n",
      "2019-10-19T20:32:31.963146: Epoch   0 Batch 1820/3125   train_loss = 1.197\n",
      "2019-10-19T20:32:32.442067: Epoch   0 Batch 1840/3125   train_loss = 1.293\n",
      "2019-10-19T20:32:32.916881: Epoch   0 Batch 1860/3125   train_loss = 1.288\n",
      "2019-10-19T20:32:33.374808: Epoch   0 Batch 1880/3125   train_loss = 1.257\n",
      "2019-10-19T20:32:33.836528: Epoch   0 Batch 1900/3125   train_loss = 1.053\n",
      "2019-10-19T20:32:34.292050: Epoch   0 Batch 1920/3125   train_loss = 1.121\n",
      "2019-10-19T20:32:34.752595: Epoch   0 Batch 1940/3125   train_loss = 1.078\n",
      "2019-10-19T20:32:35.223336: Epoch   0 Batch 1960/3125   train_loss = 1.146\n",
      "2019-10-19T20:32:35.704858: Epoch   0 Batch 1980/3125   train_loss = 1.162\n",
      "2019-10-19T20:32:36.219063: Epoch   0 Batch 2000/3125   train_loss = 1.433\n",
      "2019-10-19T20:32:36.689036: Epoch   0 Batch 2020/3125   train_loss = 1.231\n",
      "2019-10-19T20:32:37.155488: Epoch   0 Batch 2040/3125   train_loss = 1.131\n",
      "2019-10-19T20:32:37.627073: Epoch   0 Batch 2060/3125   train_loss = 1.009\n",
      "2019-10-19T20:32:38.094797: Epoch   0 Batch 2080/3125   train_loss = 1.285\n",
      "2019-10-19T20:32:38.583720: Epoch   0 Batch 2100/3125   train_loss = 1.118\n",
      "2019-10-19T20:32:39.040770: Epoch   0 Batch 2120/3125   train_loss = 1.081\n",
      "2019-10-19T20:32:39.516741: Epoch   0 Batch 2140/3125   train_loss = 1.175\n",
      "2019-10-19T20:32:39.990243: Epoch   0 Batch 2160/3125   train_loss = 1.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19T20:32:40.478662: Epoch   0 Batch 2180/3125   train_loss = 1.147\n",
      "2019-10-19T20:32:40.950874: Epoch   0 Batch 2200/3125   train_loss = 1.089\n",
      "2019-10-19T20:32:41.418152: Epoch   0 Batch 2220/3125   train_loss = 1.119\n",
      "2019-10-19T20:32:41.957569: Epoch   0 Batch 2240/3125   train_loss = 0.975\n",
      "2019-10-19T20:32:42.415580: Epoch   0 Batch 2260/3125   train_loss = 1.143\n",
      "2019-10-19T20:32:42.900024: Epoch   0 Batch 2280/3125   train_loss = 1.189\n",
      "2019-10-19T20:32:43.359509: Epoch   0 Batch 2300/3125   train_loss = 1.252\n",
      "2019-10-19T20:32:43.838867: Epoch   0 Batch 2320/3125   train_loss = 1.324\n",
      "2019-10-19T20:32:44.307375: Epoch   0 Batch 2340/3125   train_loss = 1.154\n",
      "2019-10-19T20:32:44.781828: Epoch   0 Batch 2360/3125   train_loss = 1.153\n",
      "2019-10-19T20:32:45.240052: Epoch   0 Batch 2380/3125   train_loss = 1.087\n",
      "2019-10-19T20:32:45.710111: Epoch   0 Batch 2400/3125   train_loss = 1.221\n",
      "2019-10-19T20:32:46.180049: Epoch   0 Batch 2420/3125   train_loss = 1.129\n",
      "2019-10-19T20:32:46.663760: Epoch   0 Batch 2440/3125   train_loss = 1.229\n",
      "2019-10-19T20:32:47.136025: Epoch   0 Batch 2460/3125   train_loss = 1.135\n",
      "2019-10-19T20:32:47.627377: Epoch   0 Batch 2480/3125   train_loss = 1.222\n",
      "2019-10-19T20:32:48.081800: Epoch   0 Batch 2500/3125   train_loss = 1.195\n",
      "2019-10-19T20:32:48.560593: Epoch   0 Batch 2520/3125   train_loss = 1.060\n",
      "2019-10-19T20:32:49.035248: Epoch   0 Batch 2540/3125   train_loss = 1.075\n",
      "2019-10-19T20:32:49.499534: Epoch   0 Batch 2560/3125   train_loss = 0.989\n",
      "2019-10-19T20:32:49.950648: Epoch   0 Batch 2580/3125   train_loss = 1.089\n",
      "2019-10-19T20:32:50.405245: Epoch   0 Batch 2600/3125   train_loss = 1.199\n",
      "2019-10-19T20:32:50.864722: Epoch   0 Batch 2620/3125   train_loss = 1.107\n",
      "2019-10-19T20:32:51.348028: Epoch   0 Batch 2640/3125   train_loss = 1.130\n",
      "2019-10-19T20:32:51.792093: Epoch   0 Batch 2660/3125   train_loss = 1.299\n",
      "2019-10-19T20:32:52.256078: Epoch   0 Batch 2680/3125   train_loss = 1.062\n",
      "2019-10-19T20:32:52.712806: Epoch   0 Batch 2700/3125   train_loss = 1.218\n",
      "2019-10-19T20:32:53.184753: Epoch   0 Batch 2720/3125   train_loss = 1.169\n",
      "2019-10-19T20:32:53.632812: Epoch   0 Batch 2740/3125   train_loss = 1.202\n",
      "2019-10-19T20:32:54.123129: Epoch   0 Batch 2760/3125   train_loss = 1.247\n",
      "2019-10-19T20:32:54.598227: Epoch   0 Batch 2780/3125   train_loss = 1.088\n",
      "2019-10-19T20:32:55.075433: Epoch   0 Batch 2800/3125   train_loss = 1.422\n",
      "2019-10-19T20:32:55.541709: Epoch   0 Batch 2820/3125   train_loss = 1.396\n",
      "2019-10-19T20:32:56.003164: Epoch   0 Batch 2840/3125   train_loss = 1.147\n",
      "2019-10-19T20:32:56.458729: Epoch   0 Batch 2860/3125   train_loss = 1.146\n",
      "2019-10-19T20:32:56.915386: Epoch   0 Batch 2880/3125   train_loss = 1.189\n",
      "2019-10-19T20:32:57.386208: Epoch   0 Batch 2900/3125   train_loss = 1.194\n",
      "2019-10-19T20:32:57.849163: Epoch   0 Batch 2920/3125   train_loss = 1.172\n",
      "2019-10-19T20:32:58.315092: Epoch   0 Batch 2940/3125   train_loss = 1.140\n",
      "2019-10-19T20:32:58.774042: Epoch   0 Batch 2960/3125   train_loss = 1.174\n",
      "2019-10-19T20:32:59.232022: Epoch   0 Batch 2980/3125   train_loss = 1.196\n",
      "2019-10-19T20:32:59.686980: Epoch   0 Batch 3000/3125   train_loss = 1.198\n",
      "2019-10-19T20:33:00.147622: Epoch   0 Batch 3020/3125   train_loss = 1.281\n",
      "2019-10-19T20:33:00.610948: Epoch   0 Batch 3040/3125   train_loss = 1.178\n",
      "2019-10-19T20:33:01.069213: Epoch   0 Batch 3060/3125   train_loss = 1.147\n",
      "2019-10-19T20:33:01.534677: Epoch   0 Batch 3080/3125   train_loss = 1.327\n",
      "2019-10-19T20:33:01.994131: Epoch   0 Batch 3100/3125   train_loss = 1.142\n",
      "2019-10-19T20:33:02.458951: Epoch   0 Batch 3120/3125   train_loss = 1.053\n",
      "2019-10-19T20:33:02.684754: Epoch   1 Batch    0/781   test_loss = 0.965\n",
      "2019-10-19T20:33:02.856784: Epoch   1 Batch   20/781   test_loss = 1.069\n",
      "2019-10-19T20:33:03.027080: Epoch   1 Batch   40/781   test_loss = 1.055\n",
      "2019-10-19T20:33:03.190913: Epoch   1 Batch   60/781   test_loss = 1.353\n",
      "2019-10-19T20:33:03.351157: Epoch   1 Batch   80/781   test_loss = 1.354\n",
      "2019-10-19T20:33:03.519667: Epoch   1 Batch  100/781   test_loss = 1.311\n",
      "2019-10-19T20:33:03.684433: Epoch   1 Batch  120/781   test_loss = 1.222\n",
      "2019-10-19T20:33:03.832319: Epoch   1 Batch  140/781   test_loss = 1.181\n",
      "2019-10-19T20:33:04.001368: Epoch   1 Batch  160/781   test_loss = 1.336\n",
      "2019-10-19T20:33:04.169416: Epoch   1 Batch  180/781   test_loss = 1.210\n",
      "2019-10-19T20:33:04.342961: Epoch   1 Batch  200/781   test_loss = 1.144\n",
      "2019-10-19T20:33:04.504092: Epoch   1 Batch  220/781   test_loss = 0.935\n",
      "2019-10-19T20:33:04.671971: Epoch   1 Batch  240/781   test_loss = 1.119\n",
      "2019-10-19T20:33:04.836680: Epoch   1 Batch  260/781   test_loss = 1.095\n",
      "2019-10-19T20:33:05.001534: Epoch   1 Batch  280/781   test_loss = 1.377\n",
      "2019-10-19T20:33:05.163419: Epoch   1 Batch  300/781   test_loss = 1.193\n",
      "2019-10-19T20:33:05.328804: Epoch   1 Batch  320/781   test_loss = 1.267\n",
      "2019-10-19T20:33:05.489551: Epoch   1 Batch  340/781   test_loss = 0.889\n",
      "2019-10-19T20:33:05.653479: Epoch   1 Batch  360/781   test_loss = 1.257\n",
      "2019-10-19T20:33:05.817949: Epoch   1 Batch  380/781   test_loss = 1.101\n",
      "2019-10-19T20:33:05.983818: Epoch   1 Batch  400/781   test_loss = 1.056\n",
      "2019-10-19T20:33:06.149109: Epoch   1 Batch  420/781   test_loss = 1.016\n",
      "2019-10-19T20:33:06.318800: Epoch   1 Batch  440/781   test_loss = 1.194\n",
      "2019-10-19T20:33:06.471458: Epoch   1 Batch  460/781   test_loss = 1.098\n",
      "2019-10-19T20:33:06.633512: Epoch   1 Batch  480/781   test_loss = 1.045\n",
      "2019-10-19T20:33:06.796787: Epoch   1 Batch  500/781   test_loss = 0.958\n",
      "2019-10-19T20:33:06.945530: Epoch   1 Batch  520/781   test_loss = 1.198\n",
      "2019-10-19T20:33:07.098650: Epoch   1 Batch  540/781   test_loss = 1.027\n",
      "2019-10-19T20:33:07.257908: Epoch   1 Batch  560/781   test_loss = 1.273\n",
      "2019-10-19T20:33:07.423504: Epoch   1 Batch  580/781   test_loss = 1.098\n",
      "2019-10-19T20:33:07.583781: Epoch   1 Batch  600/781   test_loss = 1.187\n",
      "2019-10-19T20:33:07.744233: Epoch   1 Batch  620/781   test_loss = 1.131\n",
      "2019-10-19T20:33:07.908364: Epoch   1 Batch  640/781   test_loss = 1.256\n",
      "2019-10-19T20:33:08.071185: Epoch   1 Batch  660/781   test_loss = 1.133\n",
      "2019-10-19T20:33:08.234015: Epoch   1 Batch  680/781   test_loss = 1.413\n",
      "2019-10-19T20:33:08.405974: Epoch   1 Batch  700/781   test_loss = 1.126\n",
      "2019-10-19T20:33:08.567614: Epoch   1 Batch  720/781   test_loss = 1.303\n",
      "2019-10-19T20:33:08.728983: Epoch   1 Batch  740/781   test_loss = 1.166\n",
      "2019-10-19T20:33:08.893168: Epoch   1 Batch  760/781   test_loss = 1.172\n",
      "2019-10-19T20:33:09.056155: Epoch   1 Batch  780/781   test_loss = 1.189\n",
      "2019-10-19T20:33:09.882681: Epoch   1 Batch   15/3125   train_loss = 1.230\n",
      "2019-10-19T20:33:10.330712: Epoch   1 Batch   35/3125   train_loss = 1.158\n",
      "2019-10-19T20:33:10.789335: Epoch   1 Batch   55/3125   train_loss = 1.188\n",
      "2019-10-19T20:33:11.254830: Epoch   1 Batch   75/3125   train_loss = 1.149\n",
      "2019-10-19T20:33:11.714426: Epoch   1 Batch   95/3125   train_loss = 1.000\n",
      "2019-10-19T20:33:12.177142: Epoch   1 Batch  115/3125   train_loss = 1.238\n",
      "2019-10-19T20:33:12.650470: Epoch   1 Batch  135/3125   train_loss = 0.962\n",
      "2019-10-19T20:33:13.112661: Epoch   1 Batch  155/3125   train_loss = 1.107\n",
      "2019-10-19T20:33:13.598217: Epoch   1 Batch  175/3125   train_loss = 1.091\n",
      "2019-10-19T20:33:14.076468: Epoch   1 Batch  195/3125   train_loss = 1.212\n",
      "2019-10-19T20:33:14.551584: Epoch   1 Batch  215/3125   train_loss = 1.094\n",
      "2019-10-19T20:33:15.018938: Epoch   1 Batch  235/3125   train_loss = 1.019\n",
      "2019-10-19T20:33:15.478404: Epoch   1 Batch  255/3125   train_loss = 1.217\n",
      "2019-10-19T20:33:15.932972: Epoch   1 Batch  275/3125   train_loss = 1.011\n",
      "2019-10-19T20:33:16.408405: Epoch   1 Batch  295/3125   train_loss = 1.010\n",
      "2019-10-19T20:33:16.864839: Epoch   1 Batch  315/3125   train_loss = 1.028\n",
      "2019-10-19T20:33:17.312646: Epoch   1 Batch  335/3125   train_loss = 0.981\n",
      "2019-10-19T20:33:17.770374: Epoch   1 Batch  355/3125   train_loss = 1.141\n",
      "2019-10-19T20:33:18.221454: Epoch   1 Batch  375/3125   train_loss = 1.223\n",
      "2019-10-19T20:33:18.679811: Epoch   1 Batch  395/3125   train_loss = 1.017\n",
      "2019-10-19T20:33:19.150032: Epoch   1 Batch  415/3125   train_loss = 1.266\n",
      "2019-10-19T20:33:19.606926: Epoch   1 Batch  435/3125   train_loss = 1.106\n",
      "2019-10-19T20:33:20.090258: Epoch   1 Batch  455/3125   train_loss = 1.141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19T20:33:20.541719: Epoch   1 Batch  475/3125   train_loss = 1.171\n",
      "2019-10-19T20:33:21.010996: Epoch   1 Batch  495/3125   train_loss = 1.067\n",
      "2019-10-19T20:33:21.465282: Epoch   1 Batch  515/3125   train_loss = 1.160\n",
      "2019-10-19T20:33:21.926018: Epoch   1 Batch  535/3125   train_loss = 1.143\n",
      "2019-10-19T20:33:22.379458: Epoch   1 Batch  555/3125   train_loss = 1.260\n",
      "2019-10-19T20:33:22.856519: Epoch   1 Batch  575/3125   train_loss = 1.092\n",
      "2019-10-19T20:33:23.315832: Epoch   1 Batch  595/3125   train_loss = 1.246\n",
      "2019-10-19T20:33:23.784422: Epoch   1 Batch  615/3125   train_loss = 1.120\n",
      "2019-10-19T20:33:24.272050: Epoch   1 Batch  635/3125   train_loss = 1.095\n",
      "2019-10-19T20:33:24.776702: Epoch   1 Batch  655/3125   train_loss = 1.027\n",
      "2019-10-19T20:33:25.274763: Epoch   1 Batch  675/3125   train_loss = 0.953\n",
      "2019-10-19T20:33:25.766341: Epoch   1 Batch  695/3125   train_loss = 1.009\n",
      "2019-10-19T20:33:26.274588: Epoch   1 Batch  715/3125   train_loss = 1.045\n",
      "2019-10-19T20:33:26.767833: Epoch   1 Batch  735/3125   train_loss = 0.969\n",
      "2019-10-19T20:33:27.262969: Epoch   1 Batch  755/3125   train_loss = 1.144\n",
      "2019-10-19T20:33:27.727727: Epoch   1 Batch  775/3125   train_loss = 1.024\n",
      "2019-10-19T20:33:28.215436: Epoch   1 Batch  795/3125   train_loss = 1.186\n",
      "2019-10-19T20:33:28.705291: Epoch   1 Batch  815/3125   train_loss = 1.072\n",
      "2019-10-19T20:33:29.166057: Epoch   1 Batch  835/3125   train_loss = 1.072\n",
      "2019-10-19T20:33:29.624728: Epoch   1 Batch  855/3125   train_loss = 1.266\n",
      "2019-10-19T20:33:30.092687: Epoch   1 Batch  875/3125   train_loss = 1.208\n",
      "2019-10-19T20:33:30.540203: Epoch   1 Batch  895/3125   train_loss = 1.030\n",
      "2019-10-19T20:33:31.003901: Epoch   1 Batch  915/3125   train_loss = 1.056\n",
      "2019-10-19T20:33:31.453150: Epoch   1 Batch  935/3125   train_loss = 1.206\n",
      "2019-10-19T20:33:31.899488: Epoch   1 Batch  955/3125   train_loss = 1.210\n",
      "2019-10-19T20:33:32.355461: Epoch   1 Batch  975/3125   train_loss = 1.157\n",
      "2019-10-19T20:33:32.804203: Epoch   1 Batch  995/3125   train_loss = 0.849\n",
      "2019-10-19T20:33:33.271291: Epoch   1 Batch 1015/3125   train_loss = 1.090\n",
      "2019-10-19T20:33:33.715006: Epoch   1 Batch 1035/3125   train_loss = 1.069\n",
      "2019-10-19T20:33:34.163826: Epoch   1 Batch 1055/3125   train_loss = 1.096\n",
      "2019-10-19T20:33:34.610232: Epoch   1 Batch 1075/3125   train_loss = 1.097\n",
      "2019-10-19T20:33:35.054213: Epoch   1 Batch 1095/3125   train_loss = 1.074\n",
      "2019-10-19T20:33:35.514313: Epoch   1 Batch 1115/3125   train_loss = 1.091\n",
      "2019-10-19T20:33:35.964314: Epoch   1 Batch 1135/3125   train_loss = 1.000\n",
      "2019-10-19T20:33:36.438199: Epoch   1 Batch 1155/3125   train_loss = 1.115\n",
      "2019-10-19T20:33:36.890823: Epoch   1 Batch 1175/3125   train_loss = 1.142\n",
      "2019-10-19T20:33:37.338027: Epoch   1 Batch 1195/3125   train_loss = 1.284\n",
      "2019-10-19T20:33:37.781227: Epoch   1 Batch 1215/3125   train_loss = 0.931\n",
      "2019-10-19T20:33:38.226839: Epoch   1 Batch 1235/3125   train_loss = 1.075\n",
      "2019-10-19T20:33:38.675698: Epoch   1 Batch 1255/3125   train_loss = 0.989\n",
      "2019-10-19T20:33:39.121996: Epoch   1 Batch 1275/3125   train_loss = 1.041\n",
      "2019-10-19T20:33:39.575341: Epoch   1 Batch 1295/3125   train_loss = 1.104\n",
      "2019-10-19T20:33:40.014641: Epoch   1 Batch 1315/3125   train_loss = 1.151\n",
      "2019-10-19T20:33:40.474708: Epoch   1 Batch 1335/3125   train_loss = 1.066\n",
      "2019-10-19T20:33:40.922933: Epoch   1 Batch 1355/3125   train_loss = 1.143\n",
      "2019-10-19T20:33:41.374063: Epoch   1 Batch 1375/3125   train_loss = 1.107\n",
      "2019-10-19T20:33:41.824422: Epoch   1 Batch 1395/3125   train_loss = 1.071\n",
      "2019-10-19T20:33:42.272632: Epoch   1 Batch 1415/3125   train_loss = 1.044\n",
      "2019-10-19T20:33:42.726087: Epoch   1 Batch 1435/3125   train_loss = 1.240\n",
      "2019-10-19T20:33:43.179559: Epoch   1 Batch 1455/3125   train_loss = 1.195\n",
      "2019-10-19T20:33:43.636719: Epoch   1 Batch 1475/3125   train_loss = 1.172\n",
      "2019-10-19T20:33:44.087464: Epoch   1 Batch 1495/3125   train_loss = 1.045\n",
      "2019-10-19T20:33:44.542363: Epoch   1 Batch 1515/3125   train_loss = 0.939\n",
      "2019-10-19T20:33:44.993959: Epoch   1 Batch 1535/3125   train_loss = 0.902\n",
      "2019-10-19T20:33:45.508413: Epoch   1 Batch 1555/3125   train_loss = 1.062\n",
      "2019-10-19T20:33:45.970972: Epoch   1 Batch 1575/3125   train_loss = 1.053\n",
      "2019-10-19T20:33:46.430383: Epoch   1 Batch 1595/3125   train_loss = 1.199\n",
      "2019-10-19T20:33:46.888188: Epoch   1 Batch 1615/3125   train_loss = 1.010\n",
      "2019-10-19T20:33:47.338503: Epoch   1 Batch 1635/3125   train_loss = 1.124\n",
      "2019-10-19T20:33:47.797566: Epoch   1 Batch 1655/3125   train_loss = 1.162\n",
      "2019-10-19T20:33:48.254973: Epoch   1 Batch 1675/3125   train_loss = 0.999\n",
      "2019-10-19T20:33:48.705026: Epoch   1 Batch 1695/3125   train_loss = 1.077\n",
      "2019-10-19T20:33:49.161441: Epoch   1 Batch 1715/3125   train_loss = 1.001\n",
      "2019-10-19T20:33:49.609334: Epoch   1 Batch 1735/3125   train_loss = 1.253\n",
      "2019-10-19T20:33:50.073662: Epoch   1 Batch 1755/3125   train_loss = 1.054\n",
      "2019-10-19T20:33:50.532083: Epoch   1 Batch 1775/3125   train_loss = 1.041\n",
      "2019-10-19T20:33:50.983726: Epoch   1 Batch 1795/3125   train_loss = 1.079\n",
      "2019-10-19T20:33:51.452788: Epoch   1 Batch 1815/3125   train_loss = 0.989\n",
      "2019-10-19T20:33:51.918539: Epoch   1 Batch 1835/3125   train_loss = 1.139\n",
      "2019-10-19T20:33:52.415065: Epoch   1 Batch 1855/3125   train_loss = 0.957\n",
      "2019-10-19T20:33:52.894242: Epoch   1 Batch 1875/3125   train_loss = 1.099\n",
      "2019-10-19T20:33:53.358990: Epoch   1 Batch 1895/3125   train_loss = 0.990\n",
      "2019-10-19T20:33:53.822720: Epoch   1 Batch 1915/3125   train_loss = 0.924\n",
      "2019-10-19T20:33:54.329431: Epoch   1 Batch 1935/3125   train_loss = 1.010\n",
      "2019-10-19T20:33:54.793602: Epoch   1 Batch 1955/3125   train_loss = 0.962\n",
      "2019-10-19T20:33:55.274146: Epoch   1 Batch 1975/3125   train_loss = 1.003\n",
      "2019-10-19T20:33:55.737782: Epoch   1 Batch 1995/3125   train_loss = 1.145\n",
      "2019-10-19T20:33:56.225907: Epoch   1 Batch 2015/3125   train_loss = 1.123\n",
      "2019-10-19T20:33:56.709475: Epoch   1 Batch 2035/3125   train_loss = 1.176\n",
      "2019-10-19T20:33:57.182290: Epoch   1 Batch 2055/3125   train_loss = 0.980\n",
      "2019-10-19T20:33:57.675177: Epoch   1 Batch 2075/3125   train_loss = 1.161\n",
      "2019-10-19T20:33:58.133352: Epoch   1 Batch 2095/3125   train_loss = 1.004\n",
      "2019-10-19T20:33:58.617212: Epoch   1 Batch 2115/3125   train_loss = 1.114\n",
      "2019-10-19T20:33:59.085188: Epoch   1 Batch 2135/3125   train_loss = 1.019\n",
      "2019-10-19T20:33:59.558722: Epoch   1 Batch 2155/3125   train_loss = 1.014\n",
      "2019-10-19T20:34:00.054892: Epoch   1 Batch 2175/3125   train_loss = 1.045\n",
      "2019-10-19T20:34:00.528811: Epoch   1 Batch 2195/3125   train_loss = 1.138\n",
      "2019-10-19T20:34:01.032330: Epoch   1 Batch 2215/3125   train_loss = 1.044\n",
      "2019-10-19T20:34:01.586825: Epoch   1 Batch 2235/3125   train_loss = 1.213\n",
      "2019-10-19T20:34:02.180033: Epoch   1 Batch 2255/3125   train_loss = 1.168\n",
      "2019-10-19T20:34:02.674004: Epoch   1 Batch 2275/3125   train_loss = 0.922\n",
      "2019-10-19T20:34:03.166948: Epoch   1 Batch 2295/3125   train_loss = 1.273\n",
      "2019-10-19T20:34:03.649169: Epoch   1 Batch 2315/3125   train_loss = 1.118\n",
      "2019-10-19T20:34:04.127280: Epoch   1 Batch 2335/3125   train_loss = 1.053\n",
      "2019-10-19T20:34:04.581353: Epoch   1 Batch 2355/3125   train_loss = 1.125\n",
      "2019-10-19T20:34:05.067745: Epoch   1 Batch 2375/3125   train_loss = 1.305\n",
      "2019-10-19T20:34:05.540859: Epoch   1 Batch 2395/3125   train_loss = 1.050\n",
      "2019-10-19T20:34:06.043398: Epoch   1 Batch 2415/3125   train_loss = 1.093\n",
      "2019-10-19T20:34:06.519101: Epoch   1 Batch 2435/3125   train_loss = 0.983\n",
      "2019-10-19T20:34:06.995564: Epoch   1 Batch 2455/3125   train_loss = 1.119\n",
      "2019-10-19T20:34:07.467269: Epoch   1 Batch 2475/3125   train_loss = 1.017\n",
      "2019-10-19T20:34:07.925322: Epoch   1 Batch 2495/3125   train_loss = 1.062\n",
      "2019-10-19T20:34:08.405529: Epoch   1 Batch 2515/3125   train_loss = 1.093\n",
      "2019-10-19T20:34:08.887706: Epoch   1 Batch 2535/3125   train_loss = 1.046\n",
      "2019-10-19T20:34:09.344501: Epoch   1 Batch 2555/3125   train_loss = 0.887\n",
      "2019-10-19T20:34:09.807433: Epoch   1 Batch 2575/3125   train_loss = 0.896\n",
      "2019-10-19T20:34:10.291579: Epoch   1 Batch 2595/3125   train_loss = 0.995\n",
      "2019-10-19T20:34:10.794903: Epoch   1 Batch 2615/3125   train_loss = 1.158\n",
      "2019-10-19T20:34:11.266964: Epoch   1 Batch 2635/3125   train_loss = 0.966\n",
      "2019-10-19T20:34:11.743573: Epoch   1 Batch 2655/3125   train_loss = 1.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19T20:34:12.222537: Epoch   1 Batch 2675/3125   train_loss = 0.947\n",
      "2019-10-19T20:34:12.675536: Epoch   1 Batch 2695/3125   train_loss = 1.023\n",
      "2019-10-19T20:34:13.149291: Epoch   1 Batch 2715/3125   train_loss = 0.930\n",
      "2019-10-19T20:34:13.611462: Epoch   1 Batch 2735/3125   train_loss = 0.886\n",
      "2019-10-19T20:34:14.084661: Epoch   1 Batch 2755/3125   train_loss = 1.055\n",
      "2019-10-19T20:34:14.552884: Epoch   1 Batch 2775/3125   train_loss = 1.055\n",
      "2019-10-19T20:34:15.020965: Epoch   1 Batch 2795/3125   train_loss = 1.007\n",
      "2019-10-19T20:34:15.473068: Epoch   1 Batch 2815/3125   train_loss = 0.997\n",
      "2019-10-19T20:34:15.936283: Epoch   1 Batch 2835/3125   train_loss = 1.145\n",
      "2019-10-19T20:34:16.421845: Epoch   1 Batch 2855/3125   train_loss = 1.081\n",
      "2019-10-19T20:34:16.883327: Epoch   1 Batch 2875/3125   train_loss = 1.003\n",
      "2019-10-19T20:34:17.366493: Epoch   1 Batch 2895/3125   train_loss = 1.031\n",
      "2019-10-19T20:34:17.823170: Epoch   1 Batch 2915/3125   train_loss = 0.974\n",
      "2019-10-19T20:34:18.313879: Epoch   1 Batch 2935/3125   train_loss = 1.135\n",
      "2019-10-19T20:34:18.780638: Epoch   1 Batch 2955/3125   train_loss = 1.061\n",
      "2019-10-19T20:34:19.237860: Epoch   1 Batch 2975/3125   train_loss = 0.974\n",
      "2019-10-19T20:34:19.707824: Epoch   1 Batch 2995/3125   train_loss = 0.976\n",
      "2019-10-19T20:34:20.183598: Epoch   1 Batch 3015/3125   train_loss = 1.017\n",
      "2019-10-19T20:34:20.629987: Epoch   1 Batch 3035/3125   train_loss = 1.038\n",
      "2019-10-19T20:34:21.080537: Epoch   1 Batch 3055/3125   train_loss = 1.038\n",
      "2019-10-19T20:34:21.550320: Epoch   1 Batch 3075/3125   train_loss = 0.922\n",
      "2019-10-19T20:34:22.017082: Epoch   1 Batch 3095/3125   train_loss = 0.955\n",
      "2019-10-19T20:34:22.495391: Epoch   1 Batch 3115/3125   train_loss = 0.898\n",
      "2019-10-19T20:34:22.861413: Epoch   2 Batch   19/781   test_loss = 1.079\n",
      "2019-10-19T20:34:23.017995: Epoch   2 Batch   39/781   test_loss = 0.857\n",
      "2019-10-19T20:34:23.177822: Epoch   2 Batch   59/781   test_loss = 0.924\n",
      "2019-10-19T20:34:23.334184: Epoch   2 Batch   79/781   test_loss = 1.031\n",
      "2019-10-19T20:34:23.489811: Epoch   2 Batch   99/781   test_loss = 1.000\n",
      "2019-10-19T20:34:23.649911: Epoch   2 Batch  119/781   test_loss = 0.992\n",
      "2019-10-19T20:34:23.819964: Epoch   2 Batch  139/781   test_loss = 1.063\n",
      "2019-10-19T20:34:23.990518: Epoch   2 Batch  159/781   test_loss = 1.030\n",
      "2019-10-19T20:34:24.157825: Epoch   2 Batch  179/781   test_loss = 0.930\n",
      "2019-10-19T20:34:24.312043: Epoch   2 Batch  199/781   test_loss = 0.918\n",
      "2019-10-19T20:34:24.465632: Epoch   2 Batch  219/781   test_loss = 1.081\n",
      "2019-10-19T20:34:24.619223: Epoch   2 Batch  239/781   test_loss = 1.207\n",
      "2019-10-19T20:34:24.767425: Epoch   2 Batch  259/781   test_loss = 0.978\n",
      "2019-10-19T20:34:24.936841: Epoch   2 Batch  279/781   test_loss = 1.114\n",
      "2019-10-19T20:34:25.101401: Epoch   2 Batch  299/781   test_loss = 1.191\n",
      "2019-10-19T20:34:25.254042: Epoch   2 Batch  319/781   test_loss = 1.002\n",
      "2019-10-19T20:34:25.412569: Epoch   2 Batch  339/781   test_loss = 0.898\n",
      "2019-10-19T20:34:25.577427: Epoch   2 Batch  359/781   test_loss = 0.886\n",
      "2019-10-19T20:34:25.740453: Epoch   2 Batch  379/781   test_loss = 1.061\n",
      "2019-10-19T20:34:25.896907: Epoch   2 Batch  399/781   test_loss = 0.796\n",
      "2019-10-19T20:34:26.065683: Epoch   2 Batch  419/781   test_loss = 0.981\n",
      "2019-10-19T20:34:26.230428: Epoch   2 Batch  439/781   test_loss = 1.078\n",
      "2019-10-19T20:34:26.405450: Epoch   2 Batch  459/781   test_loss = 1.083\n",
      "2019-10-19T20:34:26.561593: Epoch   2 Batch  479/781   test_loss = 1.052\n",
      "2019-10-19T20:34:26.729937: Epoch   2 Batch  499/781   test_loss = 0.995\n",
      "2019-10-19T20:34:26.896466: Epoch   2 Batch  519/781   test_loss = 1.093\n",
      "2019-10-19T20:34:27.068282: Epoch   2 Batch  539/781   test_loss = 0.873\n",
      "2019-10-19T20:34:27.222802: Epoch   2 Batch  559/781   test_loss = 1.181\n",
      "2019-10-19T20:34:27.382391: Epoch   2 Batch  579/781   test_loss = 1.005\n",
      "2019-10-19T20:34:27.531338: Epoch   2 Batch  599/781   test_loss = 0.955\n",
      "2019-10-19T20:34:27.685787: Epoch   2 Batch  619/781   test_loss = 1.110\n",
      "2019-10-19T20:34:27.841571: Epoch   2 Batch  639/781   test_loss = 0.890\n",
      "2019-10-19T20:34:28.038820: Epoch   2 Batch  659/781   test_loss = 1.089\n",
      "2019-10-19T20:34:28.188634: Epoch   2 Batch  679/781   test_loss = 1.154\n",
      "2019-10-19T20:34:28.357034: Epoch   2 Batch  699/781   test_loss = 0.878\n",
      "2019-10-19T20:34:28.522741: Epoch   2 Batch  719/781   test_loss = 1.033\n",
      "2019-10-19T20:34:28.686505: Epoch   2 Batch  739/781   test_loss = 0.977\n",
      "2019-10-19T20:34:28.852011: Epoch   2 Batch  759/781   test_loss = 0.891\n",
      "2019-10-19T20:34:29.033585: Epoch   2 Batch  779/781   test_loss = 0.810\n",
      "2019-10-19T20:34:29.915826: Epoch   2 Batch   10/3125   train_loss = 0.954\n",
      "2019-10-19T20:34:30.416558: Epoch   2 Batch   30/3125   train_loss = 1.070\n",
      "2019-10-19T20:34:30.903997: Epoch   2 Batch   50/3125   train_loss = 1.090\n",
      "2019-10-19T20:34:31.396276: Epoch   2 Batch   70/3125   train_loss = 1.049\n",
      "2019-10-19T20:34:31.843877: Epoch   2 Batch   90/3125   train_loss = 1.061\n",
      "2019-10-19T20:34:32.298739: Epoch   2 Batch  110/3125   train_loss = 0.943\n",
      "2019-10-19T20:34:32.770630: Epoch   2 Batch  130/3125   train_loss = 0.998\n",
      "2019-10-19T20:34:33.236811: Epoch   2 Batch  150/3125   train_loss = 1.088\n",
      "2019-10-19T20:34:33.710605: Epoch   2 Batch  170/3125   train_loss = 1.016\n",
      "2019-10-19T20:34:34.167959: Epoch   2 Batch  190/3125   train_loss = 1.086\n",
      "2019-10-19T20:34:34.639853: Epoch   2 Batch  210/3125   train_loss = 0.939\n",
      "2019-10-19T20:34:35.104902: Epoch   2 Batch  230/3125   train_loss = 1.019\n",
      "2019-10-19T20:34:35.571965: Epoch   2 Batch  250/3125   train_loss = 0.974\n",
      "2019-10-19T20:34:36.042341: Epoch   2 Batch  270/3125   train_loss = 0.827\n",
      "2019-10-19T20:34:36.516893: Epoch   2 Batch  290/3125   train_loss = 1.073\n",
      "2019-10-19T20:34:37.003838: Epoch   2 Batch  310/3125   train_loss = 0.944\n",
      "2019-10-19T20:34:37.465181: Epoch   2 Batch  330/3125   train_loss = 1.025\n",
      "2019-10-19T20:34:37.924407: Epoch   2 Batch  350/3125   train_loss = 0.906\n",
      "2019-10-19T20:34:38.441027: Epoch   2 Batch  370/3125   train_loss = 1.166\n",
      "2019-10-19T20:34:38.925073: Epoch   2 Batch  390/3125   train_loss = 1.259\n",
      "2019-10-19T20:34:39.383152: Epoch   2 Batch  410/3125   train_loss = 0.892\n",
      "2019-10-19T20:34:39.856960: Epoch   2 Batch  430/3125   train_loss = 1.119\n",
      "2019-10-19T20:34:40.319354: Epoch   2 Batch  450/3125   train_loss = 0.952\n",
      "2019-10-19T20:34:40.789137: Epoch   2 Batch  470/3125   train_loss = 0.961\n",
      "2019-10-19T20:34:41.245661: Epoch   2 Batch  490/3125   train_loss = 1.028\n",
      "2019-10-19T20:34:41.738950: Epoch   2 Batch  510/3125   train_loss = 1.091\n",
      "2019-10-19T20:34:42.198202: Epoch   2 Batch  530/3125   train_loss = 0.958\n",
      "2019-10-19T20:34:42.658226: Epoch   2 Batch  550/3125   train_loss = 1.059\n",
      "2019-10-19T20:34:43.114718: Epoch   2 Batch  570/3125   train_loss = 1.037\n",
      "2019-10-19T20:34:43.568946: Epoch   2 Batch  590/3125   train_loss = 1.056\n",
      "2019-10-19T20:34:44.036357: Epoch   2 Batch  610/3125   train_loss = 0.945\n",
      "2019-10-19T20:34:44.499470: Epoch   2 Batch  630/3125   train_loss = 1.040\n",
      "2019-10-19T20:34:44.982712: Epoch   2 Batch  650/3125   train_loss = 1.026\n",
      "2019-10-19T20:34:45.431598: Epoch   2 Batch  670/3125   train_loss = 0.951\n",
      "2019-10-19T20:34:45.892981: Epoch   2 Batch  690/3125   train_loss = 0.929\n",
      "2019-10-19T20:34:46.373244: Epoch   2 Batch  710/3125   train_loss = 0.908\n",
      "2019-10-19T20:34:46.822546: Epoch   2 Batch  730/3125   train_loss = 0.818\n",
      "2019-10-19T20:34:47.303655: Epoch   2 Batch  750/3125   train_loss = 0.960\n",
      "2019-10-19T20:34:47.766116: Epoch   2 Batch  770/3125   train_loss = 0.868\n",
      "2019-10-19T20:34:48.233350: Epoch   2 Batch  790/3125   train_loss = 0.885\n",
      "2019-10-19T20:34:48.693847: Epoch   2 Batch  810/3125   train_loss = 0.835\n",
      "2019-10-19T20:34:49.178580: Epoch   2 Batch  830/3125   train_loss = 0.830\n",
      "2019-10-19T20:34:49.630020: Epoch   2 Batch  850/3125   train_loss = 1.041\n",
      "2019-10-19T20:34:50.112284: Epoch   2 Batch  870/3125   train_loss = 0.945\n",
      "2019-10-19T20:34:50.583137: Epoch   2 Batch  890/3125   train_loss = 0.962\n",
      "2019-10-19T20:34:51.036948: Epoch   2 Batch  910/3125   train_loss = 0.982\n",
      "2019-10-19T20:34:51.522420: Epoch   2 Batch  930/3125   train_loss = 1.038\n",
      "2019-10-19T20:34:51.996409: Epoch   2 Batch  950/3125   train_loss = 0.944\n",
      "2019-10-19T20:34:52.463842: Epoch   2 Batch  970/3125   train_loss = 1.102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19T20:34:52.916647: Epoch   2 Batch  990/3125   train_loss = 0.865\n",
      "2019-10-19T20:34:53.382721: Epoch   2 Batch 1010/3125   train_loss = 1.179\n",
      "2019-10-19T20:34:53.833119: Epoch   2 Batch 1030/3125   train_loss = 0.923\n",
      "2019-10-19T20:34:54.307864: Epoch   2 Batch 1050/3125   train_loss = 0.923\n",
      "2019-10-19T20:34:54.769837: Epoch   2 Batch 1070/3125   train_loss = 0.966\n",
      "2019-10-19T20:34:55.241258: Epoch   2 Batch 1090/3125   train_loss = 1.124\n",
      "2019-10-19T20:34:55.707106: Epoch   2 Batch 1110/3125   train_loss = 1.116\n",
      "2019-10-19T20:34:56.181135: Epoch   2 Batch 1130/3125   train_loss = 0.945\n",
      "2019-10-19T20:34:56.637670: Epoch   2 Batch 1150/3125   train_loss = 0.982\n",
      "2019-10-19T20:34:57.099060: Epoch   2 Batch 1170/3125   train_loss = 1.015\n",
      "2019-10-19T20:34:57.572789: Epoch   2 Batch 1190/3125   train_loss = 1.020\n",
      "2019-10-19T20:34:58.053199: Epoch   2 Batch 1210/3125   train_loss = 0.919\n",
      "2019-10-19T20:34:58.511784: Epoch   2 Batch 1230/3125   train_loss = 0.842\n",
      "2019-10-19T20:34:58.957423: Epoch   2 Batch 1250/3125   train_loss = 1.012\n",
      "2019-10-19T20:34:59.439447: Epoch   2 Batch 1270/3125   train_loss = 1.013\n",
      "2019-10-19T20:34:59.896849: Epoch   2 Batch 1290/3125   train_loss = 0.959\n",
      "2019-10-19T20:35:00.369733: Epoch   2 Batch 1310/3125   train_loss = 0.962\n",
      "2019-10-19T20:35:00.849546: Epoch   2 Batch 1330/3125   train_loss = 1.068\n",
      "2019-10-19T20:35:01.320416: Epoch   2 Batch 1350/3125   train_loss = 0.857\n",
      "2019-10-19T20:35:01.789802: Epoch   2 Batch 1370/3125   train_loss = 0.834\n",
      "2019-10-19T20:35:02.252885: Epoch   2 Batch 1390/3125   train_loss = 1.044\n",
      "2019-10-19T20:35:02.744468: Epoch   2 Batch 1410/3125   train_loss = 0.973\n",
      "2019-10-19T20:35:03.192460: Epoch   2 Batch 1430/3125   train_loss = 1.037\n",
      "2019-10-19T20:35:03.649204: Epoch   2 Batch 1450/3125   train_loss = 1.029\n",
      "2019-10-19T20:35:04.094075: Epoch   2 Batch 1470/3125   train_loss = 0.977\n",
      "2019-10-19T20:35:04.547194: Epoch   2 Batch 1490/3125   train_loss = 1.039\n",
      "2019-10-19T20:35:05.002276: Epoch   2 Batch 1510/3125   train_loss = 0.976\n",
      "2019-10-19T20:35:05.453821: Epoch   2 Batch 1530/3125   train_loss = 1.011\n",
      "2019-10-19T20:35:05.902658: Epoch   2 Batch 1550/3125   train_loss = 0.941\n",
      "2019-10-19T20:35:06.353446: Epoch   2 Batch 1570/3125   train_loss = 0.961\n",
      "2019-10-19T20:35:06.820261: Epoch   2 Batch 1590/3125   train_loss = 0.984\n",
      "2019-10-19T20:35:07.267270: Epoch   2 Batch 1610/3125   train_loss = 1.022\n",
      "2019-10-19T20:35:07.728325: Epoch   2 Batch 1630/3125   train_loss = 1.022\n",
      "2019-10-19T20:35:08.184605: Epoch   2 Batch 1650/3125   train_loss = 0.875\n",
      "2019-10-19T20:35:08.641240: Epoch   2 Batch 1670/3125   train_loss = 0.833\n",
      "2019-10-19T20:35:09.087538: Epoch   2 Batch 1690/3125   train_loss = 1.020\n",
      "2019-10-19T20:35:09.534751: Epoch   2 Batch 1710/3125   train_loss = 0.963\n",
      "2019-10-19T20:35:09.979751: Epoch   2 Batch 1730/3125   train_loss = 0.998\n",
      "2019-10-19T20:35:10.431166: Epoch   2 Batch 1750/3125   train_loss = 0.881\n",
      "2019-10-19T20:35:10.885087: Epoch   2 Batch 1770/3125   train_loss = 1.124\n",
      "2019-10-19T20:35:11.329211: Epoch   2 Batch 1790/3125   train_loss = 1.044\n",
      "2019-10-19T20:35:11.797428: Epoch   2 Batch 1810/3125   train_loss = 1.023\n",
      "2019-10-19T20:35:12.239454: Epoch   2 Batch 1830/3125   train_loss = 1.042\n",
      "2019-10-19T20:35:12.702795: Epoch   2 Batch 1850/3125   train_loss = 0.959\n",
      "2019-10-19T20:35:13.159030: Epoch   2 Batch 1870/3125   train_loss = 1.015\n",
      "2019-10-19T20:35:13.616140: Epoch   2 Batch 1890/3125   train_loss = 0.806\n",
      "2019-10-19T20:35:14.064289: Epoch   2 Batch 1910/3125   train_loss = 0.897\n",
      "2019-10-19T20:35:14.511510: Epoch   2 Batch 1930/3125   train_loss = 1.027\n",
      "2019-10-19T20:35:14.969996: Epoch   2 Batch 1950/3125   train_loss = 0.875\n",
      "2019-10-19T20:35:15.416663: Epoch   2 Batch 1970/3125   train_loss = 0.930\n",
      "2019-10-19T20:35:15.873510: Epoch   2 Batch 1990/3125   train_loss = 0.888\n",
      "2019-10-19T20:35:16.338630: Epoch   2 Batch 2010/3125   train_loss = 0.830\n",
      "2019-10-19T20:35:16.793187: Epoch   2 Batch 2030/3125   train_loss = 0.926\n",
      "2019-10-19T20:35:17.245191: Epoch   2 Batch 2050/3125   train_loss = 0.923\n",
      "2019-10-19T20:35:17.688126: Epoch   2 Batch 2070/3125   train_loss = 0.956\n",
      "2019-10-19T20:35:18.151364: Epoch   2 Batch 2090/3125   train_loss = 0.913\n",
      "2019-10-19T20:35:18.598221: Epoch   2 Batch 2110/3125   train_loss = 1.053\n",
      "2019-10-19T20:35:19.056108: Epoch   2 Batch 2130/3125   train_loss = 0.924\n",
      "2019-10-19T20:35:19.503836: Epoch   2 Batch 2150/3125   train_loss = 0.952\n",
      "2019-10-19T20:35:19.961231: Epoch   2 Batch 2170/3125   train_loss = 0.859\n",
      "2019-10-19T20:35:20.413957: Epoch   2 Batch 2190/3125   train_loss = 0.977\n",
      "2019-10-19T20:35:20.872331: Epoch   2 Batch 2210/3125   train_loss = 0.920\n",
      "2019-10-19T20:35:21.322127: Epoch   2 Batch 2230/3125   train_loss = 0.875\n",
      "2019-10-19T20:35:21.773900: Epoch   2 Batch 2250/3125   train_loss = 0.971\n",
      "2019-10-19T20:35:22.236881: Epoch   2 Batch 2270/3125   train_loss = 0.909\n",
      "2019-10-19T20:35:22.693051: Epoch   2 Batch 2290/3125   train_loss = 0.861\n",
      "2019-10-19T20:35:23.141452: Epoch   2 Batch 2310/3125   train_loss = 0.894\n",
      "2019-10-19T20:35:23.594350: Epoch   2 Batch 2330/3125   train_loss = 1.033\n",
      "2019-10-19T20:35:24.054229: Epoch   2 Batch 2350/3125   train_loss = 1.025\n",
      "2019-10-19T20:35:24.495089: Epoch   2 Batch 2370/3125   train_loss = 0.920\n",
      "2019-10-19T20:35:24.948602: Epoch   2 Batch 2390/3125   train_loss = 1.007\n",
      "2019-10-19T20:35:25.394049: Epoch   2 Batch 2410/3125   train_loss = 1.039\n",
      "2019-10-19T20:35:25.849900: Epoch   2 Batch 2430/3125   train_loss = 0.955\n",
      "2019-10-19T20:35:26.322654: Epoch   2 Batch 2450/3125   train_loss = 0.967\n",
      "2019-10-19T20:35:26.770458: Epoch   2 Batch 2470/3125   train_loss = 0.997\n",
      "2019-10-19T20:35:27.229935: Epoch   2 Batch 2490/3125   train_loss = 1.109\n",
      "2019-10-19T20:35:27.677558: Epoch   2 Batch 2510/3125   train_loss = 1.121\n",
      "2019-10-19T20:35:28.131810: Epoch   2 Batch 2530/3125   train_loss = 0.801\n",
      "2019-10-19T20:35:28.583330: Epoch   2 Batch 2550/3125   train_loss = 1.069\n",
      "2019-10-19T20:35:29.062598: Epoch   2 Batch 2570/3125   train_loss = 1.080\n",
      "2019-10-19T20:35:29.510604: Epoch   2 Batch 2590/3125   train_loss = 1.037\n",
      "2019-10-19T20:35:29.964222: Epoch   2 Batch 2610/3125   train_loss = 0.995\n",
      "2019-10-19T20:35:30.426103: Epoch   2 Batch 2630/3125   train_loss = 0.682\n",
      "2019-10-19T20:35:30.880659: Epoch   2 Batch 2650/3125   train_loss = 0.971\n",
      "2019-10-19T20:35:31.337652: Epoch   2 Batch 2670/3125   train_loss = 0.994\n",
      "2019-10-19T20:35:31.789602: Epoch   2 Batch 2690/3125   train_loss = 1.015\n",
      "2019-10-19T20:35:32.238603: Epoch   2 Batch 2710/3125   train_loss = 0.850\n",
      "2019-10-19T20:35:32.689399: Epoch   2 Batch 2730/3125   train_loss = 1.089\n",
      "2019-10-19T20:35:33.143048: Epoch   2 Batch 2750/3125   train_loss = 1.068\n",
      "2019-10-19T20:35:33.595159: Epoch   2 Batch 2770/3125   train_loss = 0.935\n",
      "2019-10-19T20:35:34.044199: Epoch   2 Batch 2790/3125   train_loss = 0.886\n",
      "2019-10-19T20:35:34.500757: Epoch   2 Batch 2810/3125   train_loss = 0.895\n",
      "2019-10-19T20:35:34.955595: Epoch   2 Batch 2830/3125   train_loss = 0.892\n",
      "2019-10-19T20:35:35.408183: Epoch   2 Batch 2850/3125   train_loss = 0.970\n",
      "2019-10-19T20:35:35.857246: Epoch   2 Batch 2870/3125   train_loss = 0.840\n",
      "2019-10-19T20:35:36.332119: Epoch   2 Batch 2890/3125   train_loss = 0.810\n",
      "2019-10-19T20:35:36.779232: Epoch   2 Batch 2910/3125   train_loss = 0.949\n",
      "2019-10-19T20:35:37.230255: Epoch   2 Batch 2930/3125   train_loss = 0.781\n",
      "2019-10-19T20:35:37.685860: Epoch   2 Batch 2950/3125   train_loss = 1.119\n",
      "2019-10-19T20:35:38.126276: Epoch   2 Batch 2970/3125   train_loss = 0.927\n",
      "2019-10-19T20:35:38.583716: Epoch   2 Batch 2990/3125   train_loss = 0.862\n",
      "2019-10-19T20:35:39.036032: Epoch   2 Batch 3010/3125   train_loss = 0.935\n",
      "2019-10-19T20:35:39.495533: Epoch   2 Batch 3030/3125   train_loss = 0.933\n",
      "2019-10-19T20:35:39.943335: Epoch   2 Batch 3050/3125   train_loss = 0.946\n",
      "2019-10-19T20:35:40.396638: Epoch   2 Batch 3070/3125   train_loss = 0.848\n",
      "2019-10-19T20:35:40.848474: Epoch   2 Batch 3090/3125   train_loss = 0.855\n",
      "2019-10-19T20:35:41.301254: Epoch   2 Batch 3110/3125   train_loss = 0.831\n",
      "2019-10-19T20:35:41.767064: Epoch   3 Batch   18/781   test_loss = 0.777\n",
      "2019-10-19T20:35:41.928547: Epoch   3 Batch   38/781   test_loss = 0.883\n",
      "2019-10-19T20:35:42.089905: Epoch   3 Batch   58/781   test_loss = 0.887\n",
      "2019-10-19T20:35:42.251231: Epoch   3 Batch   78/781   test_loss = 0.864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19T20:35:42.411127: Epoch   3 Batch   98/781   test_loss = 0.963\n",
      "2019-10-19T20:35:42.586495: Epoch   3 Batch  118/781   test_loss = 0.843\n",
      "2019-10-19T20:35:42.756915: Epoch   3 Batch  138/781   test_loss = 0.995\n",
      "2019-10-19T20:35:42.924807: Epoch   3 Batch  158/781   test_loss = 0.863\n",
      "2019-10-19T20:35:43.094283: Epoch   3 Batch  178/781   test_loss = 0.807\n",
      "2019-10-19T20:35:43.265112: Epoch   3 Batch  198/781   test_loss = 0.965\n",
      "2019-10-19T20:35:43.436999: Epoch   3 Batch  218/781   test_loss = 1.033\n",
      "2019-10-19T20:35:43.606593: Epoch   3 Batch  238/781   test_loss = 0.980\n",
      "2019-10-19T20:35:43.768233: Epoch   3 Batch  258/781   test_loss = 0.965\n",
      "2019-10-19T20:35:43.929377: Epoch   3 Batch  278/781   test_loss = 1.069\n",
      "2019-10-19T20:35:44.087767: Epoch   3 Batch  298/781   test_loss = 0.933\n",
      "2019-10-19T20:35:44.246356: Epoch   3 Batch  318/781   test_loss = 0.900\n",
      "2019-10-19T20:35:44.407202: Epoch   3 Batch  338/781   test_loss = 0.942\n",
      "2019-10-19T20:35:44.568477: Epoch   3 Batch  358/781   test_loss = 0.930\n",
      "2019-10-19T20:35:44.729471: Epoch   3 Batch  378/781   test_loss = 0.892\n",
      "2019-10-19T20:35:44.891614: Epoch   3 Batch  398/781   test_loss = 0.781\n",
      "2019-10-19T20:35:45.053424: Epoch   3 Batch  418/781   test_loss = 0.983\n",
      "2019-10-19T20:35:45.210724: Epoch   3 Batch  438/781   test_loss = 0.997\n",
      "2019-10-19T20:35:45.370588: Epoch   3 Batch  458/781   test_loss = 0.890\n",
      "2019-10-19T20:35:45.537432: Epoch   3 Batch  478/781   test_loss = 0.948\n",
      "2019-10-19T20:35:45.700137: Epoch   3 Batch  498/781   test_loss = 0.828\n",
      "2019-10-19T20:35:45.860710: Epoch   3 Batch  518/781   test_loss = 0.940\n",
      "2019-10-19T20:35:46.019985: Epoch   3 Batch  538/781   test_loss = 0.825\n",
      "2019-10-19T20:35:46.182506: Epoch   3 Batch  558/781   test_loss = 0.857\n",
      "2019-10-19T20:35:46.343708: Epoch   3 Batch  578/781   test_loss = 0.967\n",
      "2019-10-19T20:35:46.502690: Epoch   3 Batch  598/781   test_loss = 1.156\n",
      "2019-10-19T20:35:46.678899: Epoch   3 Batch  618/781   test_loss = 0.837\n",
      "2019-10-19T20:35:46.848313: Epoch   3 Batch  638/781   test_loss = 0.838\n",
      "2019-10-19T20:35:47.018434: Epoch   3 Batch  658/781   test_loss = 1.053\n",
      "2019-10-19T20:35:47.190486: Epoch   3 Batch  678/781   test_loss = 0.933\n",
      "2019-10-19T20:35:47.361326: Epoch   3 Batch  698/781   test_loss = 0.904\n",
      "2019-10-19T20:35:47.531075: Epoch   3 Batch  718/781   test_loss = 1.005\n",
      "2019-10-19T20:35:47.698325: Epoch   3 Batch  738/781   test_loss = 0.900\n",
      "2019-10-19T20:35:47.868243: Epoch   3 Batch  758/781   test_loss = 1.002\n",
      "2019-10-19T20:35:48.014034: Epoch   3 Batch  778/781   test_loss = 0.977\n",
      "2019-10-19T20:35:48.623966: Epoch   3 Batch    5/3125   train_loss = 0.963\n",
      "2019-10-19T20:35:49.073290: Epoch   3 Batch   25/3125   train_loss = 0.961\n",
      "2019-10-19T20:35:49.531803: Epoch   3 Batch   45/3125   train_loss = 0.857\n",
      "2019-10-19T20:35:49.989764: Epoch   3 Batch   65/3125   train_loss = 0.976\n",
      "2019-10-19T20:35:50.439996: Epoch   3 Batch   85/3125   train_loss = 0.847\n",
      "2019-10-19T20:35:50.906274: Epoch   3 Batch  105/3125   train_loss = 0.756\n",
      "2019-10-19T20:35:51.361868: Epoch   3 Batch  125/3125   train_loss = 0.932\n",
      "2019-10-19T20:35:51.814386: Epoch   3 Batch  145/3125   train_loss = 0.924\n",
      "2019-10-19T20:35:52.267206: Epoch   3 Batch  165/3125   train_loss = 0.909\n",
      "2019-10-19T20:35:52.725075: Epoch   3 Batch  185/3125   train_loss = 0.819\n",
      "2019-10-19T20:35:53.171917: Epoch   3 Batch  205/3125   train_loss = 0.810\n",
      "2019-10-19T20:35:53.619273: Epoch   3 Batch  225/3125   train_loss = 0.800\n",
      "2019-10-19T20:35:54.070608: Epoch   3 Batch  245/3125   train_loss = 1.091\n",
      "2019-10-19T20:35:54.529036: Epoch   3 Batch  265/3125   train_loss = 0.884\n",
      "2019-10-19T20:35:54.982613: Epoch   3 Batch  285/3125   train_loss = 0.919\n",
      "2019-10-19T20:35:55.432607: Epoch   3 Batch  305/3125   train_loss = 0.863\n",
      "2019-10-19T20:35:55.903257: Epoch   3 Batch  325/3125   train_loss = 0.975\n",
      "2019-10-19T20:35:56.375284: Epoch   3 Batch  345/3125   train_loss = 0.960\n",
      "2019-10-19T20:35:56.832183: Epoch   3 Batch  365/3125   train_loss = 0.862\n",
      "2019-10-19T20:35:57.292471: Epoch   3 Batch  385/3125   train_loss = 0.852\n",
      "2019-10-19T20:35:57.746152: Epoch   3 Batch  405/3125   train_loss = 0.920\n",
      "2019-10-19T20:35:58.201098: Epoch   3 Batch  425/3125   train_loss = 0.966\n",
      "2019-10-19T20:35:58.650369: Epoch   3 Batch  445/3125   train_loss = 1.003\n",
      "2019-10-19T20:35:59.105066: Epoch   3 Batch  465/3125   train_loss = 0.892\n",
      "2019-10-19T20:35:59.558863: Epoch   3 Batch  485/3125   train_loss = 0.940\n",
      "2019-10-19T20:36:00.017331: Epoch   3 Batch  505/3125   train_loss = 0.845\n",
      "2019-10-19T20:36:00.465991: Epoch   3 Batch  525/3125   train_loss = 0.923\n",
      "2019-10-19T20:36:00.916229: Epoch   3 Batch  545/3125   train_loss = 0.897\n",
      "2019-10-19T20:36:01.380101: Epoch   3 Batch  565/3125   train_loss = 1.046\n",
      "2019-10-19T20:36:01.825296: Epoch   3 Batch  585/3125   train_loss = 0.883\n",
      "2019-10-19T20:36:02.286595: Epoch   3 Batch  605/3125   train_loss = 0.908\n",
      "2019-10-19T20:36:02.740273: Epoch   3 Batch  625/3125   train_loss = 0.902\n",
      "2019-10-19T20:36:03.194066: Epoch   3 Batch  645/3125   train_loss = 0.951\n",
      "2019-10-19T20:36:03.635433: Epoch   3 Batch  665/3125   train_loss = 1.021\n",
      "2019-10-19T20:36:04.080066: Epoch   3 Batch  685/3125   train_loss = 0.932\n",
      "2019-10-19T20:36:04.533809: Epoch   3 Batch  705/3125   train_loss = 1.052\n",
      "2019-10-19T20:36:04.976818: Epoch   3 Batch  725/3125   train_loss = 0.870\n",
      "2019-10-19T20:36:05.446016: Epoch   3 Batch  745/3125   train_loss = 0.883\n",
      "2019-10-19T20:36:05.894564: Epoch   3 Batch  765/3125   train_loss = 0.886\n",
      "2019-10-19T20:36:06.364689: Epoch   3 Batch  785/3125   train_loss = 1.043\n",
      "2019-10-19T20:36:06.815938: Epoch   3 Batch  805/3125   train_loss = 0.815\n",
      "2019-10-19T20:36:07.277743: Epoch   3 Batch  825/3125   train_loss = 0.909\n",
      "2019-10-19T20:36:07.727001: Epoch   3 Batch  845/3125   train_loss = 0.949\n",
      "2019-10-19T20:36:08.180394: Epoch   3 Batch  865/3125   train_loss = 1.049\n",
      "2019-10-19T20:36:08.638472: Epoch   3 Batch  885/3125   train_loss = 0.973\n",
      "2019-10-19T20:36:09.083827: Epoch   3 Batch  905/3125   train_loss = 0.990\n",
      "2019-10-19T20:36:09.537857: Epoch   3 Batch  925/3125   train_loss = 0.933\n",
      "2019-10-19T20:36:09.986303: Epoch   3 Batch  945/3125   train_loss = 1.009\n",
      "2019-10-19T20:36:10.441433: Epoch   3 Batch  965/3125   train_loss = 0.802\n",
      "2019-10-19T20:36:10.891968: Epoch   3 Batch  985/3125   train_loss = 0.961\n",
      "2019-10-19T20:36:11.382100: Epoch   3 Batch 1005/3125   train_loss = 0.798\n",
      "2019-10-19T20:36:11.825637: Epoch   3 Batch 1025/3125   train_loss = 0.907\n",
      "2019-10-19T20:36:12.279511: Epoch   3 Batch 1045/3125   train_loss = 1.159\n",
      "2019-10-19T20:36:12.732475: Epoch   3 Batch 1065/3125   train_loss = 0.927\n",
      "2019-10-19T20:36:13.179241: Epoch   3 Batch 1085/3125   train_loss = 0.801\n",
      "2019-10-19T20:36:13.640071: Epoch   3 Batch 1105/3125   train_loss = 0.875\n",
      "2019-10-19T20:36:14.084077: Epoch   3 Batch 1125/3125   train_loss = 0.855\n",
      "2019-10-19T20:36:14.532032: Epoch   3 Batch 1145/3125   train_loss = 0.923\n",
      "2019-10-19T20:36:14.990568: Epoch   3 Batch 1165/3125   train_loss = 1.003\n",
      "2019-10-19T20:36:15.442977: Epoch   3 Batch 1185/3125   train_loss = 0.822\n",
      "2019-10-19T20:36:15.891099: Epoch   3 Batch 1205/3125   train_loss = 0.858\n",
      "2019-10-19T20:36:16.381096: Epoch   3 Batch 1225/3125   train_loss = 1.006\n",
      "2019-10-19T20:36:16.846041: Epoch   3 Batch 1245/3125   train_loss = 1.060\n",
      "2019-10-19T20:36:17.318427: Epoch   3 Batch 1265/3125   train_loss = 0.917\n",
      "2019-10-19T20:36:17.776949: Epoch   3 Batch 1285/3125   train_loss = 1.030\n",
      "2019-10-19T20:36:18.221060: Epoch   3 Batch 1305/3125   train_loss = 0.788\n",
      "2019-10-19T20:36:18.678288: Epoch   3 Batch 1325/3125   train_loss = 0.889\n",
      "2019-10-19T20:36:19.121657: Epoch   3 Batch 1345/3125   train_loss = 0.973\n",
      "2019-10-19T20:36:19.569339: Epoch   3 Batch 1365/3125   train_loss = 0.776\n",
      "2019-10-19T20:36:20.030202: Epoch   3 Batch 1385/3125   train_loss = 0.810\n",
      "2019-10-19T20:36:20.481937: Epoch   3 Batch 1405/3125   train_loss = 0.885\n",
      "2019-10-19T20:36:20.934725: Epoch   3 Batch 1425/3125   train_loss = 0.984\n",
      "2019-10-19T20:36:21.389949: Epoch   3 Batch 1445/3125   train_loss = 0.999\n",
      "2019-10-19T20:36:21.850259: Epoch   3 Batch 1465/3125   train_loss = 0.883\n",
      "2019-10-19T20:36:22.297598: Epoch   3 Batch 1485/3125   train_loss = 0.946\n",
      "2019-10-19T20:36:22.756651: Epoch   3 Batch 1505/3125   train_loss = 0.758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19T20:36:23.210027: Epoch   3 Batch 1525/3125   train_loss = 0.841\n",
      "2019-10-19T20:36:23.678418: Epoch   3 Batch 1545/3125   train_loss = 0.916\n",
      "2019-10-19T20:36:24.127363: Epoch   3 Batch 1565/3125   train_loss = 0.939\n",
      "2019-10-19T20:36:24.572465: Epoch   3 Batch 1585/3125   train_loss = 0.814\n",
      "2019-10-19T20:36:25.026628: Epoch   3 Batch 1605/3125   train_loss = 0.900\n",
      "2019-10-19T20:36:25.472812: Epoch   3 Batch 1625/3125   train_loss = 0.969\n",
      "2019-10-19T20:36:25.935059: Epoch   3 Batch 1645/3125   train_loss = 1.010\n",
      "2019-10-19T20:36:26.398918: Epoch   3 Batch 1665/3125   train_loss = 0.923\n",
      "2019-10-19T20:36:26.860357: Epoch   3 Batch 1685/3125   train_loss = 0.970\n",
      "2019-10-19T20:36:27.312904: Epoch   3 Batch 1705/3125   train_loss = 0.928\n",
      "2019-10-19T20:36:27.765544: Epoch   3 Batch 1725/3125   train_loss = 0.796\n",
      "2019-10-19T20:36:28.226012: Epoch   3 Batch 1745/3125   train_loss = 0.791\n",
      "2019-10-19T20:36:28.672408: Epoch   3 Batch 1765/3125   train_loss = 0.811\n",
      "2019-10-19T20:36:29.163172: Epoch   3 Batch 1785/3125   train_loss = 1.068\n",
      "2019-10-19T20:36:29.613006: Epoch   3 Batch 1805/3125   train_loss = 0.979\n",
      "2019-10-19T20:36:30.068323: Epoch   3 Batch 1825/3125   train_loss = 1.055\n",
      "2019-10-19T20:36:30.516049: Epoch   3 Batch 1845/3125   train_loss = 0.901\n",
      "2019-10-19T20:36:30.970683: Epoch   3 Batch 1865/3125   train_loss = 0.806\n",
      "2019-10-19T20:36:31.415684: Epoch   3 Batch 1885/3125   train_loss = 0.998\n",
      "2019-10-19T20:36:31.865174: Epoch   3 Batch 1905/3125   train_loss = 0.868\n",
      "2019-10-19T20:36:32.325477: Epoch   3 Batch 1925/3125   train_loss = 0.872\n",
      "2019-10-19T20:36:32.771860: Epoch   3 Batch 1945/3125   train_loss = 0.897\n",
      "2019-10-19T20:36:33.227910: Epoch   3 Batch 1965/3125   train_loss = 0.851\n",
      "2019-10-19T20:36:33.673538: Epoch   3 Batch 1985/3125   train_loss = 0.933\n",
      "2019-10-19T20:36:34.138058: Epoch   3 Batch 2005/3125   train_loss = 0.828\n",
      "2019-10-19T20:36:34.588967: Epoch   3 Batch 2025/3125   train_loss = 0.890\n",
      "2019-10-19T20:36:35.041408: Epoch   3 Batch 2045/3125   train_loss = 0.772\n",
      "2019-10-19T20:36:35.486088: Epoch   3 Batch 2065/3125   train_loss = 0.781\n",
      "2019-10-19T20:36:35.935833: Epoch   3 Batch 2085/3125   train_loss = 0.999\n",
      "2019-10-19T20:36:36.401256: Epoch   3 Batch 2105/3125   train_loss = 0.906\n",
      "2019-10-19T20:36:36.843371: Epoch   3 Batch 2125/3125   train_loss = 0.965\n",
      "2019-10-19T20:36:37.305026: Epoch   3 Batch 2145/3125   train_loss = 0.986\n",
      "2019-10-19T20:36:37.755579: Epoch   3 Batch 2165/3125   train_loss = 0.850\n",
      "2019-10-19T20:36:38.200703: Epoch   3 Batch 2185/3125   train_loss = 0.953\n",
      "2019-10-19T20:36:38.647111: Epoch   3 Batch 2205/3125   train_loss = 0.943\n",
      "2019-10-19T20:36:39.099280: Epoch   3 Batch 2225/3125   train_loss = 0.882\n",
      "2019-10-19T20:36:39.552055: Epoch   3 Batch 2245/3125   train_loss = 0.767\n",
      "2019-10-19T20:36:40.003662: Epoch   3 Batch 2265/3125   train_loss = 0.973\n",
      "2019-10-19T20:36:40.465556: Epoch   3 Batch 2285/3125   train_loss = 1.052\n",
      "2019-10-19T20:36:40.911079: Epoch   3 Batch 2305/3125   train_loss = 0.846\n",
      "2019-10-19T20:36:41.369189: Epoch   3 Batch 2325/3125   train_loss = 0.834\n",
      "2019-10-19T20:36:41.814548: Epoch   3 Batch 2345/3125   train_loss = 0.900\n",
      "2019-10-19T20:36:42.273268: Epoch   3 Batch 2365/3125   train_loss = 0.743\n",
      "2019-10-19T20:36:42.715195: Epoch   3 Batch 2385/3125   train_loss = 0.942\n",
      "2019-10-19T20:36:43.161072: Epoch   3 Batch 2405/3125   train_loss = 0.917\n",
      "2019-10-19T20:36:43.619124: Epoch   3 Batch 2425/3125   train_loss = 0.840\n",
      "2019-10-19T20:36:44.070268: Epoch   3 Batch 2445/3125   train_loss = 0.992\n",
      "2019-10-19T20:36:44.529672: Epoch   3 Batch 2465/3125   train_loss = 0.767\n",
      "2019-10-19T20:36:44.980453: Epoch   3 Batch 2485/3125   train_loss = 0.791\n",
      "2019-10-19T20:36:45.432808: Epoch   3 Batch 2505/3125   train_loss = 0.878\n",
      "2019-10-19T20:36:45.887446: Epoch   3 Batch 2525/3125   train_loss = 0.841\n",
      "2019-10-19T20:36:46.358367: Epoch   3 Batch 2545/3125   train_loss = 0.998\n",
      "2019-10-19T20:36:46.807317: Epoch   3 Batch 2565/3125   train_loss = 0.843\n",
      "2019-10-19T20:36:47.253049: Epoch   3 Batch 2585/3125   train_loss = 0.826\n",
      "2019-10-19T20:36:47.716096: Epoch   3 Batch 2605/3125   train_loss = 0.839\n",
      "2019-10-19T20:36:48.166546: Epoch   3 Batch 2625/3125   train_loss = 1.008\n",
      "2019-10-19T20:36:48.624272: Epoch   3 Batch 2645/3125   train_loss = 0.945\n",
      "2019-10-19T20:36:49.078279: Epoch   3 Batch 2665/3125   train_loss = 0.973\n",
      "2019-10-19T20:36:49.536400: Epoch   3 Batch 2685/3125   train_loss = 0.960\n",
      "2019-10-19T20:36:49.982906: Epoch   3 Batch 2705/3125   train_loss = 0.803\n",
      "2019-10-19T20:36:50.418282: Epoch   3 Batch 2725/3125   train_loss = 0.962\n",
      "2019-10-19T20:36:50.870959: Epoch   3 Batch 2745/3125   train_loss = 0.868\n",
      "2019-10-19T20:36:51.320122: Epoch   3 Batch 2765/3125   train_loss = 0.797\n",
      "2019-10-19T20:36:51.777190: Epoch   3 Batch 2785/3125   train_loss = 0.970\n",
      "2019-10-19T20:36:52.220663: Epoch   3 Batch 2805/3125   train_loss = 0.862\n",
      "2019-10-19T20:36:52.683328: Epoch   3 Batch 2825/3125   train_loss = 0.869\n",
      "2019-10-19T20:36:53.135188: Epoch   3 Batch 2845/3125   train_loss = 0.838\n",
      "2019-10-19T20:36:53.585471: Epoch   3 Batch 2865/3125   train_loss = 0.844\n",
      "2019-10-19T20:36:54.035078: Epoch   3 Batch 2885/3125   train_loss = 0.942\n",
      "2019-10-19T20:36:54.487920: Epoch   3 Batch 2905/3125   train_loss = 0.890\n",
      "2019-10-19T20:36:54.945552: Epoch   3 Batch 2925/3125   train_loss = 0.834\n",
      "2019-10-19T20:36:55.398581: Epoch   3 Batch 2945/3125   train_loss = 0.941\n",
      "2019-10-19T20:36:55.857942: Epoch   3 Batch 2965/3125   train_loss = 1.004\n",
      "2019-10-19T20:36:56.325800: Epoch   3 Batch 2985/3125   train_loss = 0.846\n",
      "2019-10-19T20:36:56.789504: Epoch   3 Batch 3005/3125   train_loss = 0.843\n",
      "2019-10-19T20:36:57.233005: Epoch   3 Batch 3025/3125   train_loss = 0.870\n",
      "2019-10-19T20:36:57.689227: Epoch   3 Batch 3045/3125   train_loss = 0.928\n",
      "2019-10-19T20:36:58.139010: Epoch   3 Batch 3065/3125   train_loss = 0.824\n",
      "2019-10-19T20:36:58.585816: Epoch   3 Batch 3085/3125   train_loss = 0.839\n",
      "2019-10-19T20:36:59.045697: Epoch   3 Batch 3105/3125   train_loss = 0.940\n",
      "2019-10-19T20:36:59.600264: Epoch   4 Batch   17/781   test_loss = 0.891\n",
      "2019-10-19T20:36:59.772000: Epoch   4 Batch   37/781   test_loss = 0.875\n",
      "2019-10-19T20:36:59.933768: Epoch   4 Batch   57/781   test_loss = 1.002\n",
      "2019-10-19T20:37:00.107148: Epoch   4 Batch   77/781   test_loss = 0.871\n",
      "2019-10-19T20:37:00.276635: Epoch   4 Batch   97/781   test_loss = 0.753\n",
      "2019-10-19T20:37:00.446391: Epoch   4 Batch  117/781   test_loss = 0.976\n",
      "2019-10-19T20:37:00.615530: Epoch   4 Batch  137/781   test_loss = 0.894\n",
      "2019-10-19T20:37:00.793670: Epoch   4 Batch  157/781   test_loss = 0.939\n",
      "2019-10-19T20:37:00.962572: Epoch   4 Batch  177/781   test_loss = 0.880\n",
      "2019-10-19T20:37:01.133651: Epoch   4 Batch  197/781   test_loss = 0.900\n",
      "2019-10-19T20:37:01.303865: Epoch   4 Batch  217/781   test_loss = 0.700\n",
      "2019-10-19T20:37:01.468063: Epoch   4 Batch  237/781   test_loss = 0.766\n",
      "2019-10-19T20:37:01.629972: Epoch   4 Batch  257/781   test_loss = 1.005\n",
      "2019-10-19T20:37:01.796780: Epoch   4 Batch  277/781   test_loss = 0.972\n",
      "2019-10-19T20:37:01.958241: Epoch   4 Batch  297/781   test_loss = 0.951\n",
      "2019-10-19T20:37:02.117539: Epoch   4 Batch  317/781   test_loss = 1.054\n",
      "2019-10-19T20:37:02.279618: Epoch   4 Batch  337/781   test_loss = 0.918\n",
      "2019-10-19T20:37:02.441860: Epoch   4 Batch  357/781   test_loss = 0.925\n",
      "2019-10-19T20:37:02.600542: Epoch   4 Batch  377/781   test_loss = 0.966\n",
      "2019-10-19T20:37:02.763831: Epoch   4 Batch  397/781   test_loss = 0.915\n",
      "2019-10-19T20:37:02.934995: Epoch   4 Batch  417/781   test_loss = 0.823\n",
      "2019-10-19T20:37:03.093310: Epoch   4 Batch  437/781   test_loss = 0.808\n",
      "2019-10-19T20:37:03.255520: Epoch   4 Batch  457/781   test_loss = 0.710\n",
      "2019-10-19T20:37:03.418050: Epoch   4 Batch  477/781   test_loss = 0.901\n",
      "2019-10-19T20:37:03.578326: Epoch   4 Batch  497/781   test_loss = 0.808\n",
      "2019-10-19T20:37:03.738189: Epoch   4 Batch  517/781   test_loss = 0.834\n",
      "2019-10-19T20:37:03.917160: Epoch   4 Batch  537/781   test_loss = 0.906\n",
      "2019-10-19T20:37:04.084795: Epoch   4 Batch  557/781   test_loss = 1.014\n",
      "2019-10-19T20:37:04.255848: Epoch   4 Batch  577/781   test_loss = 0.899\n",
      "2019-10-19T20:37:04.426318: Epoch   4 Batch  597/781   test_loss = 0.828\n",
      "2019-10-19T20:37:04.594518: Epoch   4 Batch  617/781   test_loss = 0.831\n",
      "2019-10-19T20:37:04.762770: Epoch   4 Batch  637/781   test_loss = 0.829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19T20:37:04.934817: Epoch   4 Batch  657/781   test_loss = 1.087\n",
      "2019-10-19T20:37:05.103036: Epoch   4 Batch  677/781   test_loss = 0.951\n",
      "2019-10-19T20:37:05.271011: Epoch   4 Batch  697/781   test_loss = 0.937\n",
      "2019-10-19T20:37:05.442915: Epoch   4 Batch  717/781   test_loss = 0.880\n",
      "2019-10-19T20:37:05.606696: Epoch   4 Batch  737/781   test_loss = 0.767\n",
      "2019-10-19T20:37:05.766802: Epoch   4 Batch  757/781   test_loss = 1.048\n",
      "2019-10-19T20:37:05.935331: Epoch   4 Batch  777/781   test_loss = 0.926\n",
      "2019-10-19T20:37:06.411524: Epoch   4 Batch    0/3125   train_loss = 0.991\n",
      "2019-10-19T20:37:06.867423: Epoch   4 Batch   20/3125   train_loss = 0.850\n",
      "2019-10-19T20:37:07.330970: Epoch   4 Batch   40/3125   train_loss = 0.877\n",
      "2019-10-19T20:37:07.778546: Epoch   4 Batch   60/3125   train_loss = 0.725\n",
      "2019-10-19T20:37:08.232994: Epoch   4 Batch   80/3125   train_loss = 0.863\n",
      "2019-10-19T20:37:08.682357: Epoch   4 Batch  100/3125   train_loss = 0.911\n",
      "2019-10-19T20:37:09.137680: Epoch   4 Batch  120/3125   train_loss = 0.984\n",
      "2019-10-19T20:37:09.585093: Epoch   4 Batch  140/3125   train_loss = 0.925\n",
      "2019-10-19T20:37:10.044260: Epoch   4 Batch  160/3125   train_loss = 0.815\n",
      "2019-10-19T20:37:10.489087: Epoch   4 Batch  180/3125   train_loss = 0.854\n",
      "2019-10-19T20:37:10.941275: Epoch   4 Batch  200/3125   train_loss = 1.104\n",
      "2019-10-19T20:37:11.400172: Epoch   4 Batch  220/3125   train_loss = 0.884\n",
      "2019-10-19T20:37:11.852978: Epoch   4 Batch  240/3125   train_loss = 0.964\n",
      "2019-10-19T20:37:12.311186: Epoch   4 Batch  260/3125   train_loss = 0.966\n",
      "2019-10-19T20:37:12.759071: Epoch   4 Batch  280/3125   train_loss = 0.965\n",
      "2019-10-19T20:37:13.215121: Epoch   4 Batch  300/3125   train_loss = 1.103\n",
      "2019-10-19T20:37:13.658296: Epoch   4 Batch  320/3125   train_loss = 0.999\n",
      "2019-10-19T20:37:14.115861: Epoch   4 Batch  340/3125   train_loss = 0.727\n",
      "2019-10-19T20:37:14.562988: Epoch   4 Batch  360/3125   train_loss = 0.794\n",
      "2019-10-19T20:37:15.012138: Epoch   4 Batch  380/3125   train_loss = 0.890\n",
      "2019-10-19T20:37:15.468070: Epoch   4 Batch  400/3125   train_loss = 0.869\n",
      "2019-10-19T20:37:15.921474: Epoch   4 Batch  420/3125   train_loss = 0.784\n",
      "2019-10-19T20:37:16.393798: Epoch   4 Batch  440/3125   train_loss = 0.856\n",
      "2019-10-19T20:37:16.845281: Epoch   4 Batch  460/3125   train_loss = 0.847\n",
      "2019-10-19T20:37:17.301278: Epoch   4 Batch  480/3125   train_loss = 1.034\n",
      "2019-10-19T20:37:17.753905: Epoch   4 Batch  500/3125   train_loss = 0.702\n",
      "2019-10-19T20:37:18.215064: Epoch   4 Batch  520/3125   train_loss = 0.915\n",
      "2019-10-19T20:37:18.664737: Epoch   4 Batch  540/3125   train_loss = 0.810\n",
      "2019-10-19T20:37:19.120588: Epoch   4 Batch  560/3125   train_loss = 0.980\n",
      "2019-10-19T20:37:19.576643: Epoch   4 Batch  580/3125   train_loss = 1.012\n",
      "2019-10-19T20:37:20.024596: Epoch   4 Batch  600/3125   train_loss = 0.918\n",
      "2019-10-19T20:37:20.479190: Epoch   4 Batch  620/3125   train_loss = 0.877\n",
      "2019-10-19T20:37:20.933330: Epoch   4 Batch  640/3125   train_loss = 0.880\n",
      "2019-10-19T20:37:21.399150: Epoch   4 Batch  660/3125   train_loss = 0.889\n",
      "2019-10-19T20:37:21.880841: Epoch   4 Batch  680/3125   train_loss = 0.937\n",
      "2019-10-19T20:37:22.337055: Epoch   4 Batch  700/3125   train_loss = 0.897\n",
      "2019-10-19T20:37:22.794298: Epoch   4 Batch  720/3125   train_loss = 0.808\n",
      "2019-10-19T20:37:23.242360: Epoch   4 Batch  740/3125   train_loss = 0.905\n",
      "2019-10-19T20:37:23.713166: Epoch   4 Batch  760/3125   train_loss = 0.808\n",
      "2019-10-19T20:37:24.161132: Epoch   4 Batch  780/3125   train_loss = 0.959\n",
      "2019-10-19T20:37:24.610240: Epoch   4 Batch  800/3125   train_loss = 0.791\n",
      "2019-10-19T20:37:25.062982: Epoch   4 Batch  820/3125   train_loss = 0.875\n",
      "2019-10-19T20:37:25.516449: Epoch   4 Batch  840/3125   train_loss = 0.770\n",
      "2019-10-19T20:37:25.977553: Epoch   4 Batch  860/3125   train_loss = 0.854\n",
      "2019-10-19T20:37:26.445466: Epoch   4 Batch  880/3125   train_loss = 0.769\n",
      "2019-10-19T20:37:26.905881: Epoch   4 Batch  900/3125   train_loss = 0.865\n",
      "2019-10-19T20:37:27.349968: Epoch   4 Batch  920/3125   train_loss = 0.960\n",
      "2019-10-19T20:37:27.807283: Epoch   4 Batch  940/3125   train_loss = 0.893\n",
      "2019-10-19T20:37:28.252586: Epoch   4 Batch  960/3125   train_loss = 0.965\n",
      "2019-10-19T20:37:28.705476: Epoch   4 Batch  980/3125   train_loss = 0.983\n",
      "2019-10-19T20:37:29.177901: Epoch   4 Batch 1000/3125   train_loss = 0.965\n",
      "2019-10-19T20:37:29.623193: Epoch   4 Batch 1020/3125   train_loss = 0.912\n",
      "2019-10-19T20:37:30.076211: Epoch   4 Batch 1040/3125   train_loss = 0.818\n",
      "2019-10-19T20:37:30.536953: Epoch   4 Batch 1060/3125   train_loss = 0.956\n",
      "2019-10-19T20:37:30.989548: Epoch   4 Batch 1080/3125   train_loss = 0.937\n",
      "2019-10-19T20:37:31.442863: Epoch   4 Batch 1100/3125   train_loss = 0.881\n",
      "2019-10-19T20:37:31.897087: Epoch   4 Batch 1120/3125   train_loss = 0.864\n",
      "2019-10-19T20:37:32.348760: Epoch   4 Batch 1140/3125   train_loss = 0.845\n",
      "2019-10-19T20:37:32.805159: Epoch   4 Batch 1160/3125   train_loss = 0.806\n",
      "2019-10-19T20:37:33.262925: Epoch   4 Batch 1180/3125   train_loss = 0.891\n",
      "2019-10-19T20:37:33.711832: Epoch   4 Batch 1200/3125   train_loss = 0.964\n",
      "2019-10-19T20:37:34.166767: Epoch   4 Batch 1220/3125   train_loss = 0.954\n",
      "2019-10-19T20:37:34.614554: Epoch   4 Batch 1240/3125   train_loss = 0.755\n",
      "2019-10-19T20:37:35.070315: Epoch   4 Batch 1260/3125   train_loss = 0.900\n",
      "2019-10-19T20:37:35.517827: Epoch   4 Batch 1280/3125   train_loss = 0.857\n",
      "2019-10-19T20:37:35.970466: Epoch   4 Batch 1300/3125   train_loss = 0.875\n",
      "2019-10-19T20:37:36.437928: Epoch   4 Batch 1320/3125   train_loss = 0.887\n",
      "2019-10-19T20:37:36.901946: Epoch   4 Batch 1340/3125   train_loss = 0.755\n",
      "2019-10-19T20:37:37.346717: Epoch   4 Batch 1360/3125   train_loss = 0.788\n",
      "2019-10-19T20:37:37.799142: Epoch   4 Batch 1380/3125   train_loss = 0.787\n",
      "2019-10-19T20:37:38.248630: Epoch   4 Batch 1400/3125   train_loss = 0.932\n",
      "2019-10-19T20:37:38.698432: Epoch   4 Batch 1420/3125   train_loss = 0.937\n",
      "2019-10-19T20:37:39.144231: Epoch   4 Batch 1440/3125   train_loss = 0.779\n",
      "2019-10-19T20:37:39.592474: Epoch   4 Batch 1460/3125   train_loss = 0.912\n",
      "2019-10-19T20:37:40.061331: Epoch   4 Batch 1480/3125   train_loss = 0.890\n",
      "2019-10-19T20:37:40.510872: Epoch   4 Batch 1500/3125   train_loss = 0.857\n",
      "2019-10-19T20:37:40.968440: Epoch   4 Batch 1520/3125   train_loss = 0.825\n",
      "2019-10-19T20:37:41.411662: Epoch   4 Batch 1540/3125   train_loss = 0.986\n",
      "2019-10-19T20:37:41.872169: Epoch   4 Batch 1560/3125   train_loss = 0.822\n",
      "2019-10-19T20:37:42.325712: Epoch   4 Batch 1580/3125   train_loss = 0.968\n",
      "2019-10-19T20:37:42.774933: Epoch   4 Batch 1600/3125   train_loss = 0.816\n",
      "2019-10-19T20:37:43.232208: Epoch   4 Batch 1620/3125   train_loss = 0.821\n",
      "2019-10-19T20:37:43.687971: Epoch   4 Batch 1640/3125   train_loss = 0.950\n",
      "2019-10-19T20:37:44.138582: Epoch   4 Batch 1660/3125   train_loss = 0.959\n",
      "2019-10-19T20:37:44.584784: Epoch   4 Batch 1680/3125   train_loss = 0.872\n",
      "2019-10-19T20:37:45.042888: Epoch   4 Batch 1700/3125   train_loss = 0.806\n",
      "2019-10-19T20:37:45.496718: Epoch   4 Batch 1720/3125   train_loss = 0.855\n",
      "2019-10-19T20:37:45.949672: Epoch   4 Batch 1740/3125   train_loss = 0.987\n",
      "2019-10-19T20:37:46.421801: Epoch   4 Batch 1760/3125   train_loss = 0.889\n",
      "2019-10-19T20:37:46.875806: Epoch   4 Batch 1780/3125   train_loss = 0.888\n",
      "2019-10-19T20:37:47.328468: Epoch   4 Batch 1800/3125   train_loss = 0.817\n",
      "2019-10-19T20:37:47.779074: Epoch   4 Batch 1820/3125   train_loss = 0.856\n",
      "2019-10-19T20:37:48.239222: Epoch   4 Batch 1840/3125   train_loss = 0.912\n",
      "2019-10-19T20:37:48.745350: Epoch   4 Batch 1860/3125   train_loss = 0.933\n",
      "2019-10-19T20:37:49.207708: Epoch   4 Batch 1880/3125   train_loss = 0.865\n",
      "2019-10-19T20:37:49.659076: Epoch   4 Batch 1900/3125   train_loss = 0.734\n",
      "2019-10-19T20:37:50.122103: Epoch   4 Batch 1920/3125   train_loss = 0.863\n",
      "2019-10-19T20:37:50.577257: Epoch   4 Batch 1940/3125   train_loss = 0.747\n",
      "2019-10-19T20:37:51.030157: Epoch   4 Batch 1960/3125   train_loss = 0.746\n",
      "2019-10-19T20:37:51.484731: Epoch   4 Batch 1980/3125   train_loss = 0.894\n",
      "2019-10-19T20:37:51.933782: Epoch   4 Batch 2000/3125   train_loss = 1.034\n",
      "2019-10-19T20:37:52.396988: Epoch   4 Batch 2020/3125   train_loss = 0.948\n",
      "2019-10-19T20:37:52.837807: Epoch   4 Batch 2040/3125   train_loss = 0.754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19T20:37:53.296236: Epoch   4 Batch 2060/3125   train_loss = 0.797\n",
      "2019-10-19T20:37:53.743059: Epoch   4 Batch 2080/3125   train_loss = 0.986\n",
      "2019-10-19T20:37:54.189682: Epoch   4 Batch 2100/3125   train_loss = 0.827\n",
      "2019-10-19T20:37:54.647314: Epoch   4 Batch 2120/3125   train_loss = 0.837\n",
      "2019-10-19T20:37:55.095070: Epoch   4 Batch 2140/3125   train_loss = 0.824\n",
      "2019-10-19T20:37:55.556953: Epoch   4 Batch 2160/3125   train_loss = 0.819\n",
      "2019-10-19T20:37:56.002813: Epoch   4 Batch 2180/3125   train_loss = 0.926\n",
      "2019-10-19T20:37:56.468157: Epoch   4 Batch 2200/3125   train_loss = 0.778\n",
      "2019-10-19T20:37:56.909657: Epoch   4 Batch 2220/3125   train_loss = 0.808\n",
      "2019-10-19T20:37:57.372338: Epoch   4 Batch 2240/3125   train_loss = 0.827\n",
      "2019-10-19T20:37:57.824343: Epoch   4 Batch 2260/3125   train_loss = 0.892\n",
      "2019-10-19T20:37:58.272637: Epoch   4 Batch 2280/3125   train_loss = 0.901\n",
      "2019-10-19T20:37:58.725712: Epoch   4 Batch 2300/3125   train_loss = 0.890\n",
      "2019-10-19T20:37:59.180631: Epoch   4 Batch 2320/3125   train_loss = 0.960\n",
      "2019-10-19T20:37:59.637120: Epoch   4 Batch 2340/3125   train_loss = 0.841\n",
      "2019-10-19T20:38:00.075175: Epoch   4 Batch 2360/3125   train_loss = 0.858\n",
      "2019-10-19T20:38:00.525607: Epoch   4 Batch 2380/3125   train_loss = 0.820\n",
      "2019-10-19T20:38:00.978567: Epoch   4 Batch 2400/3125   train_loss = 0.938\n",
      "2019-10-19T20:38:01.425620: Epoch   4 Batch 2420/3125   train_loss = 0.782\n",
      "2019-10-19T20:38:01.884552: Epoch   4 Batch 2440/3125   train_loss = 0.791\n",
      "2019-10-19T20:38:02.333070: Epoch   4 Batch 2460/3125   train_loss = 0.905\n",
      "2019-10-19T20:38:02.789238: Epoch   4 Batch 2480/3125   train_loss = 0.956\n",
      "2019-10-19T20:38:03.234626: Epoch   4 Batch 2500/3125   train_loss = 0.857\n",
      "2019-10-19T20:38:03.702216: Epoch   4 Batch 2520/3125   train_loss = 0.948\n",
      "2019-10-19T20:38:04.152707: Epoch   4 Batch 2540/3125   train_loss = 0.823\n",
      "2019-10-19T20:38:04.604151: Epoch   4 Batch 2560/3125   train_loss = 0.694\n",
      "2019-10-19T20:38:05.050885: Epoch   4 Batch 2580/3125   train_loss = 0.855\n",
      "2019-10-19T20:38:05.498882: Epoch   4 Batch 2600/3125   train_loss = 0.892\n",
      "2019-10-19T20:38:05.959368: Epoch   4 Batch 2620/3125   train_loss = 0.808\n",
      "2019-10-19T20:38:06.417546: Epoch   4 Batch 2640/3125   train_loss = 0.848\n",
      "2019-10-19T20:38:06.884014: Epoch   4 Batch 2660/3125   train_loss = 1.046\n",
      "2019-10-19T20:38:07.338984: Epoch   4 Batch 2680/3125   train_loss = 0.805\n",
      "2019-10-19T20:38:07.790620: Epoch   4 Batch 2700/3125   train_loss = 0.886\n",
      "2019-10-19T20:38:08.247588: Epoch   4 Batch 2720/3125   train_loss = 0.804\n",
      "2019-10-19T20:38:08.695290: Epoch   4 Batch 2740/3125   train_loss = 0.843\n",
      "2019-10-19T20:38:09.150987: Epoch   4 Batch 2760/3125   train_loss = 0.784\n",
      "2019-10-19T20:38:09.605428: Epoch   4 Batch 2780/3125   train_loss = 0.815\n",
      "2019-10-19T20:38:10.050395: Epoch   4 Batch 2800/3125   train_loss = 1.072\n",
      "2019-10-19T20:38:10.510109: Epoch   4 Batch 2820/3125   train_loss = 1.046\n",
      "2019-10-19T20:38:10.960434: Epoch   4 Batch 2840/3125   train_loss = 0.842\n",
      "2019-10-19T20:38:11.441422: Epoch   4 Batch 2860/3125   train_loss = 0.768\n",
      "2019-10-19T20:38:11.891178: Epoch   4 Batch 2880/3125   train_loss = 0.842\n",
      "2019-10-19T20:38:12.345749: Epoch   4 Batch 2900/3125   train_loss = 0.869\n",
      "2019-10-19T20:38:12.793272: Epoch   4 Batch 2920/3125   train_loss = 0.818\n",
      "2019-10-19T20:38:13.238018: Epoch   4 Batch 2940/3125   train_loss = 0.922\n",
      "2019-10-19T20:38:13.694000: Epoch   4 Batch 2960/3125   train_loss = 0.899\n",
      "2019-10-19T20:38:14.152109: Epoch   4 Batch 2980/3125   train_loss = 0.871\n",
      "2019-10-19T20:38:14.608376: Epoch   4 Batch 3000/3125   train_loss = 0.958\n",
      "2019-10-19T20:38:15.066919: Epoch   4 Batch 3020/3125   train_loss = 1.035\n",
      "2019-10-19T20:38:15.524241: Epoch   4 Batch 3040/3125   train_loss = 0.929\n",
      "2019-10-19T20:38:15.973520: Epoch   4 Batch 3060/3125   train_loss = 0.789\n",
      "2019-10-19T20:38:16.437064: Epoch   4 Batch 3080/3125   train_loss = 1.047\n",
      "2019-10-19T20:38:16.886199: Epoch   4 Batch 3100/3125   train_loss = 1.007\n",
      "2019-10-19T20:38:17.337247: Epoch   4 Batch 3120/3125   train_loss = 0.832\n",
      "2019-10-19T20:38:17.561484: Epoch   5 Batch   16/781   test_loss = 0.840\n",
      "2019-10-19T20:38:17.731327: Epoch   5 Batch   36/781   test_loss = 0.915\n",
      "2019-10-19T20:38:17.895151: Epoch   5 Batch   56/781   test_loss = 0.941\n",
      "2019-10-19T20:38:18.058338: Epoch   5 Batch   76/781   test_loss = 0.965\n",
      "2019-10-19T20:38:18.216080: Epoch   5 Batch   96/781   test_loss = 1.035\n",
      "2019-10-19T20:38:18.373909: Epoch   5 Batch  116/781   test_loss = 0.865\n",
      "2019-10-19T20:38:18.536881: Epoch   5 Batch  136/781   test_loss = 0.831\n",
      "2019-10-19T20:38:18.698482: Epoch   5 Batch  156/781   test_loss = 0.921\n",
      "2019-10-19T20:38:18.859789: Epoch   5 Batch  176/781   test_loss = 0.845\n",
      "2019-10-19T20:38:19.021458: Epoch   5 Batch  196/781   test_loss = 0.792\n",
      "2019-10-19T20:38:19.181023: Epoch   5 Batch  216/781   test_loss = 0.977\n",
      "2019-10-19T20:38:19.339840: Epoch   5 Batch  236/781   test_loss = 0.825\n",
      "2019-10-19T20:38:19.513980: Epoch   5 Batch  256/781   test_loss = 0.832\n",
      "2019-10-19T20:38:19.683949: Epoch   5 Batch  276/781   test_loss = 1.104\n",
      "2019-10-19T20:38:19.848411: Epoch   5 Batch  296/781   test_loss = 0.869\n",
      "2019-10-19T20:38:20.008746: Epoch   5 Batch  316/781   test_loss = 0.862\n",
      "2019-10-19T20:38:20.170092: Epoch   5 Batch  336/781   test_loss = 0.761\n",
      "2019-10-19T20:38:20.327711: Epoch   5 Batch  356/781   test_loss = 0.854\n",
      "2019-10-19T20:38:20.499092: Epoch   5 Batch  376/781   test_loss = 0.919\n",
      "2019-10-19T20:38:20.659626: Epoch   5 Batch  396/781   test_loss = 0.859\n",
      "2019-10-19T20:38:20.818956: Epoch   5 Batch  416/781   test_loss = 0.956\n",
      "2019-10-19T20:38:20.977575: Epoch   5 Batch  436/781   test_loss = 0.935\n",
      "2019-10-19T20:38:21.139283: Epoch   5 Batch  456/781   test_loss = 0.715\n",
      "2019-10-19T20:38:21.298617: Epoch   5 Batch  476/781   test_loss = 0.948\n",
      "2019-10-19T20:38:21.460322: Epoch   5 Batch  496/781   test_loss = 0.985\n",
      "2019-10-19T20:38:21.633250: Epoch   5 Batch  516/781   test_loss = 0.809\n",
      "2019-10-19T20:38:21.802972: Epoch   5 Batch  536/781   test_loss = 0.954\n",
      "2019-10-19T20:38:21.971482: Epoch   5 Batch  556/781   test_loss = 0.831\n",
      "2019-10-19T20:38:22.141261: Epoch   5 Batch  576/781   test_loss = 0.956\n",
      "2019-10-19T20:38:22.309626: Epoch   5 Batch  596/781   test_loss = 1.000\n",
      "2019-10-19T20:38:22.479032: Epoch   5 Batch  616/781   test_loss = 0.977\n",
      "2019-10-19T20:38:22.655689: Epoch   5 Batch  636/781   test_loss = 0.841\n",
      "2019-10-19T20:38:22.827211: Epoch   5 Batch  656/781   test_loss = 0.875\n",
      "2019-10-19T20:38:22.995058: Epoch   5 Batch  676/781   test_loss = 1.104\n",
      "2019-10-19T20:38:23.154855: Epoch   5 Batch  696/781   test_loss = 0.874\n",
      "2019-10-19T20:38:23.315899: Epoch   5 Batch  716/781   test_loss = 0.900\n",
      "2019-10-19T20:38:23.475705: Epoch   5 Batch  736/781   test_loss = 1.070\n",
      "2019-10-19T20:38:23.649981: Epoch   5 Batch  756/781   test_loss = 0.838\n",
      "2019-10-19T20:38:23.803619: Epoch   5 Batch  776/781   test_loss = 0.771\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "     \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        #将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "                                                           targets_values,  \n",
    "                                                           test_size = 0.2,  \n",
    "                                                           random_state = 0)  \n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "\n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)  #\n",
    "            \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "                \n",
    "        #使用测试数据的迭代\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 TensorBoard 中查看可视化结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir /PATH_TO_CODE/runs/1513402825/summaries/\n",
    "\n",
    "<img src=\"assets/loss.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存参数\n",
    "保存`save_dir` 在生成预测时使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params((save_dir))\n",
    "\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显示训练Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 249,
       "width": 370
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显示测试Loss\n",
    "迭代次数再增加一些，下降的趋势会明显一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取 Tensors\n",
    "使用函数 [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name)从 `loaded_graph` 中获取tensors，后面的推荐功能要用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "#     inference = loaded_graph.get_tensor_by_name(\"inference/inference/BiasAdd:0\")\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\") # 之前是MatMul:0 因为inference代码修改了 这里也要修改 感谢网友 @清歌 指出问题\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定用户和电影进行评分\n",
    "这部分就是对网络做正向传播，计算得到预测的评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[3.1968396]], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成Movie特征矩阵\n",
    "将训练好的电影特征组合成电影特征矩阵并保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save_model/save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('./save_data/movie_matrics.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_matrics = pickle.load(open('./save_data/movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成User特征矩阵\n",
    "将训练好的用户特征组合成用户特征矩阵并保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save_model/save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('./save_data/users_matrics.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrics = pickle.load(open('./save_data/users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始推荐电影\n",
    "使用生产的用户特征矩阵和电影特征矩阵做电影推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推荐同类型的电影\n",
    "思路是计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的top_k个，这里加了些随机选择在里面，保证每次的推荐稍稍有些不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "\n",
    "        #推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save_model/save\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "以下是给您的推荐：\n",
      "1098\n",
      "[1114 'Funeral, The (1996)' 'Drama']\n",
      "112\n",
      "[114 \"Margaret's Museum (1995)\" 'Drama']\n",
      "145\n",
      "[147 'Basketball Diaries, The (1995)' 'Drama']\n",
      "404\n",
      "[408 '8 Seconds (1994)' 'Drama']\n",
      "2453\n",
      "[2522 \"Airport '77 (1977)\" 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{112, 145, 404, 1098, 2453}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推荐您喜欢的电影\n",
    "思路是使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的top_k个，同样加了些随机选择部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
    "\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        #推荐您喜欢的电影\n",
    "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n",
    "\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     print(sim.shape)\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "    #     sim_norm = probs_norm_similarity.eval()\n",
    "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
    "    \n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save_model/save\n",
      "以下是给您的推荐：\n",
      "1130\n",
      "[1146 \"Curtis's Charm (1995)\" 'Comedy|Drama']\n",
      "911\n",
      "[923 'Citizen Kane (1941)' 'Drama']\n",
      "914\n",
      "[926 'All About Eve (1950)' 'Drama']\n",
      "1691\n",
      "[1742 'Caught Up (1998)' 'Crime']\n",
      "637\n",
      "[642 'Roula (1995)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{637, 911, 914, 1130, 1691}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看过这个电影的人还看了（喜欢）哪些电影\n",
    "- 首先选出喜欢某个电影的top_k个人，得到这几个人的用户特征向量。\n",
    "- 然后计算这几个人对所有电影的评分\n",
    "- 选择每个人评分最高的电影作为推荐\n",
    "- 同样加入了随机选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
    "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n",
    "    #     print(normalized_users_matrics.eval().shape)\n",
    "    #     print(probs_user_favorite_similarity.eval()[0][favorite_user_id])\n",
    "    #     print(favorite_user_id.shape)\n",
    "    \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        \n",
    "        print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n",
    "        probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])\n",
    "        probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "    \n",
    "    #     print(sim.shape)\n",
    "    #     print(np.argmax(sim, 1))\n",
    "        p = np.argmax(sim, 1)\n",
    "        print(\"喜欢看这个电影的人还喜欢看：\")\n",
    "\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = p[random.randrange(top_k)]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save_model/save\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "喜欢看这个电影的人是：[[5767 'M' 25 2]\n",
      " [5102 'M' 25 12]\n",
      " [5610 'F' 25 1]\n",
      " [4142 'M' 56 7]\n",
      " [3213 'F' 25 1]\n",
      " [2316 'M' 35 0]\n",
      " [5079 'M' 25 4]\n",
      " [342 'M' 18 12]\n",
      " [100 'M' 35 17]\n",
      " [96 'F' 25 16]\n",
      " [371 'M' 18 4]\n",
      " [483 'M' 18 12]\n",
      " [2292 'M' 50 15]\n",
      " [5861 'F' 50 1]\n",
      " [2421 'M' 35 7]\n",
      " [3698 'M' 25 7]\n",
      " [2154 'M' 25 12]\n",
      " [4064 'M' 45 0]\n",
      " [5336 'M' 56 7]\n",
      " [4085 'F' 25 6]]\n",
      "喜欢看这个电影的人还喜欢看：\n",
      "900\n",
      "[912 'Casablanca (1942)' 'Drama|Romance|War']\n",
      "1773\n",
      "[1842 'Illtown (1996)' 'Crime|Drama']\n",
      "1178\n",
      "[1196 'Star Wars: Episode V - The Empire Strikes Back (1980)'\n",
      " 'Action|Adventure|Drama|Sci-Fi|War']\n",
      "1691\n",
      "[1742 'Caught Up (1998)' 'Crime']\n",
      "1180\n",
      "[1198 'Raiders of the Lost Ark (1981)' 'Action|Adventure']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{900, 1178, 1180, 1691, 1773}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论\n",
    "\n",
    "以上就是实现的常用的推荐功能，将网络模型作为回归问题进行训练，得到训练好的用户特征矩阵和电影特征矩阵进行推荐。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
